The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=50, no round_timeout
num_clients: 100
num_rounds: 50
dataset:
  name: vicgalle/alpaca-gpt4
model:
  name: openlm-research/open_llama_7b_v2
  quantization: 4
  gradient_checkpointing: true
  lora:
    peft_lora_r: 32
    peft_lora_alpha: 64
train:
  num_rounds: ${num_rounds}
  save_every_round: 5
  learning_rate_max: 5.0e-05
  learning_rate_min: 1.0e-06
  seq_length: 512
  training_arguments:
    output_dir: null
    learning_rate: null
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 1
    logging_steps: 10
    num_train_epochs: 3
    max_steps: 10
    report_to: null
    save_steps: 1000
    save_total_limit: 10
    gradient_checkpointing: ${model.gradient_checkpointing}
    lr_scheduler_type: constant
strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.1
  fraction_evaluate: 0.0
client_resources:
  num_cpus: 4
  num_gpus: 1.0

[2024-06-13 20:50:28,475][flwr][INFO] - Starting Flower simulation, config: num_rounds=50, no round_timeout
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-06-13 20:50:54,384	INFO worker.py:1752 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:A100': 1.0, 'memory': 862170356736.0, 'object_store_memory': 200000000000.0, 'node:10.100.115.7': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
[2024-06-13 20:51:02,963][flwr][INFO] - Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:A100': 1.0, 'memory': 862170356736.0, 'object_store_memory': 200000000000.0, 'node:10.100.115.7': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-06-13 20:51:02,966][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 1.0}
[2024-06-13 20:51:02,966][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 1.0}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[2024-06-13 20:51:02,998][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[92mINFO [0m:      [INIT]
[2024-06-13 20:51:03,006][flwr][INFO] - [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[2024-06-13 20:51:03,006][flwr][INFO] - Requesting initial parameters from one random client
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [02:49<02:49, 169.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:41<00:00, 100.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:41<00:00, 110.83s/it]
[92mINFO [0m:      Received initial parameters from one random client
[2024-06-13 20:56:37,045][flwr][INFO] - Received initial parameters from one random client
[92mINFO [0m:      Evaluating initial global parameters
[2024-06-13 20:56:37,051][flwr][INFO] - Evaluating initial global parameters
[92mINFO [0m:      initial parameters (loss, other metrics): 0.0, {}
[2024-06-13 20:56:37,108][flwr][INFO] - initial parameters (loss, other metrics): 0.0, {}
[92mINFO [0m:      
[2024-06-13 20:56:37,108][flwr][INFO] - 
[92mINFO [0m:      [ROUND 1]
[2024-06-13 20:56:37,108][flwr][INFO] - [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 20:56:37,109][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.57s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 808.96 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 747.11 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:26<03:57, 26.37s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:45<02:54, 21.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:59<02:09, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:18<01:50, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:36<01:32, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:53<01:12, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:12<00:54, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:29<00:36, 18.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:45<00:17, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00, 16.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00, 16.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00, 16.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00, 17.91s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2322.57 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1577.08 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:08, 18.32s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:22, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:00, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:15, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.38s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2578.54 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1571.92 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.24s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.08s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2358.94 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1896.04 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.10s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.64s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:46, 15.65s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.44s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1724.16 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1217.91 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.03s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:04, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.20s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2386.46 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1607.78 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:55, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:49, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.12s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2262.79 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1602.98 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.59s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.69s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.69s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.87s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1753.31 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1309.34 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:01, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.73s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:17, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 15.91s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 15.91s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 15.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.70s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2138.23 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1294.69 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:19, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:34, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2023.49 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1236.97 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:37, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:16, 15.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:35, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.43s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3816, 'grad_norm': 0.33250850439071655, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 179.034, 'train_samples_per_second': 0.894, 'train_steps_per_second': 0.056, 'train_loss': 1.3815662384033203, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3026, 'grad_norm': 0.35673248767852783, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1984, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.302606201171875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3382, 'grad_norm': 0.4252474009990692, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.4292, 'train_samples_per_second': 0.939, 'train_steps_per_second': 0.059, 'train_loss': 1.3381524085998535, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.4124, 'grad_norm': 0.34177541732788086, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3792, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.062, 'train_loss': 1.4123907089233398, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2966, 'grad_norm': 0.32557910680770874, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.4848, 'train_samples_per_second': 0.967, 'train_steps_per_second': 0.06, 'train_loss': 1.2966246604919434, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2422, 'grad_norm': 0.3261563181877136, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.1149, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.058, 'train_loss': 1.242215347290039, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.303, 'grad_norm': 0.35200485587120056, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.7694, 'train_samples_per_second': 0.921, 'train_steps_per_second': 0.058, 'train_loss': 1.302968692779541, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3276, 'grad_norm': 0.3773947060108185, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.124, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.058, 'train_loss': 1.3276079177856446, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3513, 'grad_norm': 0.3361337184906006, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.2173, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.058, 'train_loss': 1.3512689590454101, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.3224, 'grad_norm': 0.29461151361465454, 'learning_rate': 4.9951654846492654e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.3293, 'train_samples_per_second': 0.974, 'train_steps_per_second': 0.061, 'train_loss': 1.3223559379577636, 'epoch': 0.3}
[2024-06-13 21:28:17,472][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (1, 0.0, {}, 1901.123904607026)
[2024-06-13 21:28:18,232][flwr][INFO] - fit progress: (1, 0.0, {}, 1901.123904607026)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-13 21:28:18,232][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-13 21:28:18,232][flwr][INFO] - 
[92mINFO [0m:      [ROUND 2]
[2024-06-13 21:28:18,232][flwr][INFO] - [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 21:28:18,233][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2697.84 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1758.47 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.19s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:53, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.13s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:00, 15.13s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:15, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.97s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.97s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.97s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.78s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2631.94 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1878.65 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:09, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:40, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:31, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.64s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1474.25 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1195.64 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.55s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:23, 16.65s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:32<00:16, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2182.70 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1488.60 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.10s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:16, 15.32s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.30s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.22s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3491.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3196.30 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:16, 15.12s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.65s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:07, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:39, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:22, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:00, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.21s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3848.97 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3551.35 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:43, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:18, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3567.37 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3334.10 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:40, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:12, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:00, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3341.68 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3054.71 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:03, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.25s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.91s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3836.34 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3472.10 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:06, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:59, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:10, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.38s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2452, 'grad_norm': 0.5737147927284241, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.7676, 'train_samples_per_second': 1.061, 'train_steps_per_second': 0.066, 'train_loss': 1.245157527923584, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2282, 'grad_norm': 0.3048912286758423, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.3766, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.06, 'train_loss': 1.2282286643981934, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.232, 'grad_norm': 0.5157611966133118, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.5119, 'train_samples_per_second': 0.938, 'train_steps_per_second': 0.059, 'train_loss': 1.2320207595825194, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1927, 'grad_norm': 0.313652366399765, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.7739, 'train_samples_per_second': 1.008, 'train_steps_per_second': 0.063, 'train_loss': 1.192711639404297, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2584, 'grad_norm': 0.33881083130836487, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.9035, 'train_samples_per_second': 0.959, 'train_steps_per_second': 0.06, 'train_loss': 1.2583515167236328, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2103, 'grad_norm': 0.39024844765663147, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8737, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.210331344604492, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2212, 'grad_norm': 0.3222213685512543, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2688, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.2211569786071776, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2384, 'grad_norm': 0.30716073513031006, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.3847, 'train_samples_per_second': 0.956, 'train_steps_per_second': 0.06, 'train_loss': 1.2384453773498536, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2053, 'grad_norm': 0.383063405752182, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9481, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.2053479194641112, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2815, 'grad_norm': 0.523467481136322, 'learning_rate': 4.9806810182204715e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.8059, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.061, 'train_loss': 1.2815439224243164, 'epoch': 0.3}
[2024-06-13 21:59:12,090][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (2, 0.0, {}, 3755.832433670992)
[2024-06-13 21:59:12,941][flwr][INFO] - fit progress: (2, 0.0, {}, 3755.832433670992)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-13 21:59:12,941][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-13 21:59:12,941][flwr][INFO] - 
[92mINFO [0m:      [ROUND 3]
[2024-06-13 21:59:12,941][flwr][INFO] - [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 21:59:12,941][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:15, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3905.40 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3629.98 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:19, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.51s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3637.95 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3415.13 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2823.20 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2620.15 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:55, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3724.15 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3465.09 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:24, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3639.46 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3330.00 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3044.44 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2881.09 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.86s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:03, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.98s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2940.08 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2755.12 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1822, 'grad_norm': 0.3151741027832031, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9908, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.182236099243164, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1803, 'grad_norm': 0.38933292031288147, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6906, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.180282974243164, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2807, 'grad_norm': 0.4027600884437561, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.1779, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.2807394981384277, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1998, 'grad_norm': 0.3002851903438568, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.7905, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1998347282409667, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1369, 'grad_norm': 0.46847259998321533, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9345, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.1368556022644043, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1772, 'grad_norm': 0.3811468482017517, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9556, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.1771757125854492, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1805, 'grad_norm': 0.5965781807899475, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.6906, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.1804951667785644, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1566, 'grad_norm': 0.3996725082397461, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.8933, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.156610870361328, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2744, 'grad_norm': 0.4264119565486908, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9841, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.2743868827819824, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1967, 'grad_norm': 0.3507755696773529, 'learning_rate': 4.956603764285288e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.5967, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.1966743469238281, 'epoch': 0.3}
[2024-06-13 22:29:51,965][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (3, 0.0, {}, 5595.642689475091)
[2024-06-13 22:29:52,751][flwr][INFO] - fit progress: (3, 0.0, {}, 5595.642689475091)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-13 22:29:52,751][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-13 22:29:52,751][flwr][INFO] - 
[92mINFO [0m:      [ROUND 4]
[2024-06-13 22:29:52,751][flwr][INFO] - [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 22:29:52,751][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3627.13 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3388.96 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.96s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.67s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3438.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3221.75 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:40, 14.32s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:17, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:00, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.22s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3693.96 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3443.03 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:50, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:40, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:26, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:44, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.43s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3629.69 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3328.29 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:01, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:46, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:50, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.49s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3494.37 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3173.50 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.60s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:44, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.89s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:30, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.75s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3394.23 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3158.09 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:04, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:45, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:52, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.78s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3674.39 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3417.02 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:37, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:23, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:09, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:46, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.94s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3327.92 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2584.45 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:43, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.96s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:16, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.64s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:23, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.68s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.94s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:09, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:40, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:31, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.62s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2004, 'grad_norm': 0.3788541853427887, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.7024, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.2004228591918946, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1302, 'grad_norm': 0.549579381942749, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.2515, 'train_samples_per_second': 1.011, 'train_steps_per_second': 0.063, 'train_loss': 1.130218505859375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1781, 'grad_norm': 0.43341654539108276, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.0503, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.063, 'train_loss': 1.178143310546875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1308, 'grad_norm': 0.497791588306427, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.6863, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.130805778503418, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1991, 'grad_norm': 0.6589668393135071, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.8109, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.1991204261779784, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1697, 'grad_norm': 0.38605520129203796, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.6082, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.057, 'train_loss': 1.1697336196899415, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1684, 'grad_norm': 0.5551292896270752, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.6249, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.06, 'train_loss': 1.1683786392211915, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2171, 'grad_norm': 0.5503461360931396, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.3665, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.062, 'train_loss': 1.2171167373657226, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1838, 'grad_norm': 0.4703584313392639, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.9335, 'train_samples_per_second': 0.958, 'train_steps_per_second': 0.06, 'train_loss': 1.183782196044922, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1599, 'grad_norm': 0.38439416885375977, 'learning_rate': 4.9230287447651464e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.1924, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.1599368095397948, 'epoch': 0.3}
[2024-06-13 23:00:39,588][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (4, 0.0, {}, 7443.245990209049)
[2024-06-13 23:00:40,354][flwr][INFO] - fit progress: (4, 0.0, {}, 7443.245990209049)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-13 23:00:40,354][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-13 23:00:40,354][flwr][INFO] - 
[92mINFO [0m:      [ROUND 5]
[2024-06-13 23:00:40,355][flwr][INFO] - [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 23:00:40,355][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.81s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 4053.14 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3369.39 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:17, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:03, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:50, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.85s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2587.65 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1973.07 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:50, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.54s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.63s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.43s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1874.79 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1515.18 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:44, 14.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:31, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.16s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:25, 16.13s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:29, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:16, 15.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.06s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.06s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.50s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.74s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3333.90 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2174.00 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:08, 18.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.45s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:06, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.35s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.69s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2373.40 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1415.50 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:41, 16.93s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:27, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:29, 14.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:17<00:13, 13.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:34<00:00, 14.45s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:34<00:00, 14.45s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:34<00:00, 14.45s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:34<00:00, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.81s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2516.84 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1999.93 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:19, 15.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:35, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:17, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.48s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.54s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2397.16 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 1737.15 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.56s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3540.13 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3314.55 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.43s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1908, 'grad_norm': 0.36927464604377747, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.1202, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.059, 'train_loss': 1.1908469200134277, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.213, 'grad_norm': 0.3743235468864441, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.2812, 'train_samples_per_second': 1.024, 'train_steps_per_second': 0.064, 'train_loss': 1.2129694938659668, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1229, 'grad_norm': 0.46967872977256775, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4282, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1228862762451173, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1877, 'grad_norm': 0.40895235538482666, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.8887, 'train_samples_per_second': 0.982, 'train_steps_per_second': 0.061, 'train_loss': 1.187663745880127, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1862, 'grad_norm': 0.38297170400619507, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.3041, 'train_samples_per_second': 0.929, 'train_steps_per_second': 0.058, 'train_loss': 1.1862085342407227, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.167, 'grad_norm': 0.4263908565044403, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 154.1384, 'train_samples_per_second': 1.038, 'train_steps_per_second': 0.065, 'train_loss': 1.1670026779174805, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1665, 'grad_norm': 0.5027287006378174, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.3434, 'train_samples_per_second': 1.01, 'train_steps_per_second': 0.063, 'train_loss': 1.1665029525756836, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1536, 'grad_norm': 0.6565449833869934, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.9387, 'train_samples_per_second': 1.0, 'train_steps_per_second': 0.063, 'train_loss': 1.15362548828125, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2251, 'grad_norm': 0.5681260228157043, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.4815, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.056, 'train_loss': 1.225062084197998, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1667, 'grad_norm': 0.6342764496803284, 'learning_rate': 4.8800884649231264e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6717, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.1667437553405762, 'epoch': 0.3}
[2024-06-13 23:31:38,850][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  6.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.01s/it]
[92mINFO [0m:      fit progress: (5, 0.0, {}, 9319.196916416055)
[2024-06-13 23:31:56,305][flwr][INFO] - fit progress: (5, 0.0, {}, 9319.196916416055)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-13 23:31:56,306][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-13 23:31:56,306][flwr][INFO] - 
[92mINFO [0m:      [ROUND 6]
[2024-06-13 23:31:56,306][flwr][INFO] - [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-13 23:31:56,307][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.12s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.60s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.89s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2583.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2452.08 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.89s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:41, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:27, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:09, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:29, 14.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:17<00:13, 13.49s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 15.39s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2843.71 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2668.51 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.84s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2895.40 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2728.35 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.49s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.79s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2876.87 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2721.06 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.74s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2874.17 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2417.24 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:52, 14.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.80s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:01, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.24s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.57s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.83s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2804.10 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2630.74 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.61s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:58, 14.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.17s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1465, 'grad_norm': 0.4687143862247467, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6885, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.1464637756347655, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2045, 'grad_norm': 0.4916399419307709, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.8178, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.2044999122619628, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1578, 'grad_norm': 0.45548558235168457, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 153.9231, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.065, 'train_loss': 1.1578301429748534, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1487, 'grad_norm': 0.4381871819496155, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9039, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.148732566833496, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2085, 'grad_norm': 0.42471757531166077, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4518, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.2085118293762207, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1494, 'grad_norm': 0.5128375887870789, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1564, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1494043350219727, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1548, 'grad_norm': 0.46339648962020874, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0072, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1548425674438476, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1444, 'grad_norm': 0.8168314695358276, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.6778, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.1443799972534179, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1654, 'grad_norm': 0.5948998928070068, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0767, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.165385913848877, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1494, 'grad_norm': 0.37456104159355164, 'learning_rate': 4.827952390426216e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6666, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.1493526458740235, 'epoch': 0.3}
[2024-06-14 00:02:46,234][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (6, 0.0, {}, 11169.84238903108)
[2024-06-14 00:02:46,951][flwr][INFO] - fit progress: (6, 0.0, {}, 11169.84238903108)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 00:02:46,951][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 00:02:46,951][flwr][INFO] - 
[92mINFO [0m:      [ROUND 7]
[2024-06-14 00:02:46,951][flwr][INFO] - [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 00:02:46,951][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.09s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.64s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.05s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.45s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.45s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.74s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:07, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:39, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:22, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:00, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.92s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:43, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.86s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.36s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.76s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2774.40 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2571.70 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:32, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:44, 14.91s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:31, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3798.59 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3532.44 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.91s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.34s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:50, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:40, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:26, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:44, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.90s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1994, 'grad_norm': 0.4434584677219391, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4471, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1993610382080078, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1937, 'grad_norm': 0.5023751258850098, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.1781, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.193674373626709, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2096, 'grad_norm': 0.6246823668479919, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.4695, 'train_samples_per_second': 0.897, 'train_steps_per_second': 0.056, 'train_loss': 1.2095745086669922, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1218, 'grad_norm': 0.4764489531517029, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.898, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.1218053817749023, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1938, 'grad_norm': 0.6642589569091797, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.3496, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.062, 'train_loss': 1.193775463104248, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2364, 'grad_norm': 0.5136184692382812, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9927, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.2364007949829101, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0879, 'grad_norm': 0.5017890334129333, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9524, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.087935733795166, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1093, 'grad_norm': 0.5244421362876892, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.2201, 'train_samples_per_second': 0.98, 'train_steps_per_second': 0.061, 'train_loss': 1.1093469619750977, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1393, 'grad_norm': 0.42760559916496277, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4118, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.1393043518066406, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1539, 'grad_norm': 0.5104948282241821, 'learning_rate': 4.766826278541748e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.0386, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.063, 'train_loss': 1.1538939476013184, 'epoch': 0.3}
[2024-06-14 00:33:57,099][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (7, 0.0, {}, 13040.807557804044)
[2024-06-14 00:33:57,916][flwr][INFO] - fit progress: (7, 0.0, {}, 13040.807557804044)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 00:33:57,916][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 00:33:57,916][flwr][INFO] - 
[92mINFO [0m:      [ROUND 8]
[2024-06-14 00:33:57,916][flwr][INFO] - [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 00:33:57,916][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.36s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.76s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2666.47 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2521.96 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:42, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:28, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:53, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:34, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.77s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.62s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.89s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:30, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.52s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3195.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2908.94 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:58, 13.17s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:51, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:24, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:50, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:16, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2851.88 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2710.74 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.55s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:04, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:45, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:29, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:12, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:54, 18.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:36, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.74s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2856.19 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2699.50 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:25, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:30, 15.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.84s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2909.85 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2706.07 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:53, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.12s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3036.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2833.55 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<02:04, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:15, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:47, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2774.05 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2618.38 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1029, 'grad_norm': 0.5061566829681396, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 176.4453, 'train_samples_per_second': 0.907, 'train_steps_per_second': 0.057, 'train_loss': 1.1029440879821777, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1172, 'grad_norm': 0.6105048656463623, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.3103, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.058, 'train_loss': 1.1172224044799806, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1691, 'grad_norm': 0.500737190246582, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.8226, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.1691424369812011, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1673, 'grad_norm': 0.590691089630127, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8004, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1672866821289063, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1359, 'grad_norm': 0.40152350068092346, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9762, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.1358911514282226, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1735, 'grad_norm': 0.4531819820404053, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.0251, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.057, 'train_loss': 1.1734703063964844, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1739, 'grad_norm': 0.4923418164253235, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.0599, 'train_samples_per_second': 0.964, 'train_steps_per_second': 0.06, 'train_loss': 1.1738973617553712, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1227, 'grad_norm': 0.4504620134830475, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.7319, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.058, 'train_loss': 1.1227466583251953, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1514, 'grad_norm': 0.4597187638282776, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.3588, 'train_samples_per_second': 0.973, 'train_steps_per_second': 0.061, 'train_loss': 1.1514167785644531, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1806, 'grad_norm': 0.5373391509056091, 'learning_rate': 4.696951366107466e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4236, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1806013107299804, 'epoch': 0.3}
[2024-06-14 01:05:28,951][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (8, 0.0, {}, 14932.673011259176)
[2024-06-14 01:05:29,781][flwr][INFO] - fit progress: (8, 0.0, {}, 14932.673011259176)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 01:05:29,781][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 01:05:29,781][flwr][INFO] - 
[92mINFO [0m:      [ROUND 9]
[2024-06-14 01:05:29,782][flwr][INFO] - [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 01:05:29,782][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.03s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2743.62 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2599.17 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:09<01:27,  9.78s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:24, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3027.99 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2848.19 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:43, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:28, 14.70s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.07s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3392.82 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3163.81 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:46, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:30, 18.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.17s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:05<00:54, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:23<00:36, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:18, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.09s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:09, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:43, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:18, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:40, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:27, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:44, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2586.28 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2458.20 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:02, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:47, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.66s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.44s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2865.26 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2703.44 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:15, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.65s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.79s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:32, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1284, 'grad_norm': 0.4559970200061798, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.3969, 'train_samples_per_second': 0.945, 'train_steps_per_second': 0.059, 'train_loss': 1.128361701965332, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1693, 'grad_norm': 0.46701234579086304, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.5387, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'train_loss': 1.1693178176879884, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1452, 'grad_norm': 0.5500171780586243, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.2988, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.06, 'train_loss': 1.1452484130859375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1314, 'grad_norm': 0.4578527808189392, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.2437, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.057, 'train_loss': 1.1314453125, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1311, 'grad_norm': 0.4749112129211426, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2696, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1311350822448731, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1436, 'grad_norm': 0.5391055941581726, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.0637, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.063, 'train_loss': 1.1436193466186524, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.153, 'grad_norm': 0.4875430762767792, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.9875, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.058, 'train_loss': 1.1529826164245605, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1938, 'grad_norm': 0.4243013858795166, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.9217, 'train_samples_per_second': 0.964, 'train_steps_per_second': 0.06, 'train_loss': 1.1938297271728515, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1288, 'grad_norm': 0.8033650517463684, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.7129, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.128775405883789, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0776, 'grad_norm': 0.5318453907966614, 'learning_rate': 4.618603417479937e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9516, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.0775524139404298, 'epoch': 0.3}
[2024-06-14 01:36:46,114][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (9, 0.0, {}, 16809.727803208167)
[2024-06-14 01:36:46,836][flwr][INFO] - fit progress: (9, 0.0, {}, 16809.727803208167)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 01:36:46,836][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 01:36:46,836][flwr][INFO] - 
[92mINFO [0m:      [ROUND 10]
[2024-06-14 01:36:46,836][flwr][INFO] - [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 01:36:46,836][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2899.14 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2739.84 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.69s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.44s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2839.56 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2695.69 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:06, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:14, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:43<00:42, 14.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:59<00:29, 14.73s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:13<00:14, 14.65s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.24s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.82s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2850.77 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2695.40 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:54, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:36, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2981.19 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2808.84 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:24, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:43, 14.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:37, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:16, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<01:01, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:47, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.02s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.38s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.75s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3052.37 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2878.61 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.88s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.38s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.60s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:25, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:30, 15.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.13s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:44, 14.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:35, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1935, 'grad_norm': 0.4999151825904846, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0577, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.1935110092163086, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.199, 'grad_norm': 0.4686443507671356, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 152.3554, 'train_samples_per_second': 1.05, 'train_steps_per_second': 0.066, 'train_loss': 1.1990361213684082, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1564, 'grad_norm': 0.6686695218086243, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.475, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.1563837051391601, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1348, 'grad_norm': 0.6562057137489319, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.4028, 'train_samples_per_second': 1.017, 'train_steps_per_second': 0.064, 'train_loss': 1.1348010063171388, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.147, 'grad_norm': 0.6623231172561646, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7157, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1470254898071288, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1928, 'grad_norm': 0.6027348041534424, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.807, 'train_samples_per_second': 0.942, 'train_steps_per_second': 0.059, 'train_loss': 1.1928330421447755, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1223, 'grad_norm': 0.47964194416999817, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4475, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.122337532043457, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.171, 'grad_norm': 0.601282000541687, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4314, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.170976161956787, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1657, 'grad_norm': 0.5235195159912109, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.072, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.165748691558838, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1423, 'grad_norm': 0.6194107532501221, 'learning_rate': 4.532091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0187, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.063, 'train_loss': 1.1423202514648438, 'epoch': 0.3}
[2024-06-14 02:07:42,638][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.83s/it]
[92mINFO [0m:      fit progress: (10, 0.0, {}, 18684.244964098092)
[2024-06-14 02:08:01,353][flwr][INFO] - fit progress: (10, 0.0, {}, 18684.244964098092)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 02:08:01,354][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 02:08:01,354][flwr][INFO] - 
[92mINFO [0m:      [ROUND 11]
[2024-06-14 02:08:01,354][flwr][INFO] - [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 02:08:01,354][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.82s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.07s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:24, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.58s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.90s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.18s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:53, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:00, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:18<00:15, 15.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.75s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:16, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.76s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.87s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2701.13 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2562.48 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.69s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:09, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:24, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:49, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.81s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3007.36 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2834.63 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:50, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]
[36m(ClientAppActor pid=979026)[0m                                                100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.89s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2825.30 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2674.15 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.81s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:47, 18.59s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:10, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.02s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.02s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.72s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.91s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.43s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:44, 14.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:35, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2778.27 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2633.35 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:40, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1831, 'grad_norm': 0.5719757676124573, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.8113, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.1830757141113282, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.083, 'grad_norm': 0.6735507845878601, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9569, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.0829673767089845, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1411, 'grad_norm': 0.8999658823013306, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.4, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.066, 'train_loss': 1.1410728454589845, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0912, 'grad_norm': 0.5360952615737915, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.5434, 'train_samples_per_second': 1.009, 'train_steps_per_second': 0.063, 'train_loss': 1.0911853790283204, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1453, 'grad_norm': 0.4899108409881592, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.8266, 'train_samples_per_second': 0.959, 'train_steps_per_second': 0.06, 'train_loss': 1.1453369140625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1415, 'grad_norm': 0.6822385787963867, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.6292, 'train_samples_per_second': 0.938, 'train_steps_per_second': 0.059, 'train_loss': 1.1414706230163574, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1231, 'grad_norm': 0.5810359120368958, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.5782, 'train_samples_per_second': 0.949, 'train_steps_per_second': 0.059, 'train_loss': 1.1231489181518555, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1371, 'grad_norm': 0.8276399374008179, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8146, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.137132453918457, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1369, 'grad_norm': 0.6457939743995667, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.008, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.063, 'train_loss': 1.1369049072265625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1665, 'grad_norm': 0.6509302854537964, 'learning_rate': 4.437757444800684e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.5003, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.166524314880371, 'epoch': 0.3}
[2024-06-14 02:39:14,344][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (11, 0.0, {}, 20557.99082269403)
[2024-06-14 02:39:15,099][flwr][INFO] - fit progress: (11, 0.0, {}, 20557.99082269403)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 02:39:15,099][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 02:39:15,099][flwr][INFO] - 
[92mINFO [0m:      [ROUND 12]
[2024-06-14 02:39:15,099][flwr][INFO] - [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 02:39:15,100][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.22s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.65s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.62s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:22, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:17, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.70s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.82s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:42, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:02, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:34, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:49, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:16, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.05s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.88s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.10s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.51s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.08s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.52s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:43, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.14s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.62s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.25s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3014.00 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2842.72 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.69s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:37, 13.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:21, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:51, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:46, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.42s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:50, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1429, 'grad_norm': 0.5411662459373474, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.666, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.142878246307373, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1154, 'grad_norm': 0.5060330629348755, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.6042, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.115408706665039, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.163, 'grad_norm': 0.6386094689369202, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4414, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1630289077758789, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1313, 'grad_norm': 0.8671647310256958, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6725, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.1312631607055663, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.22, 'grad_norm': 0.5875650644302368, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.1763, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.2199583053588867, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1775, 'grad_norm': 0.7534143328666687, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.3228, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.062, 'train_loss': 1.1775426864624023, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1212, 'grad_norm': 0.5398989915847778, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9525, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.1211840629577636, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1251, 'grad_norm': 0.4526372253894806, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6323, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.1251092910766602, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2075, 'grad_norm': 0.7131067514419556, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.1531, 'train_samples_per_second': 0.999, 'train_steps_per_second': 0.062, 'train_loss': 1.2074893951416015, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0953, 'grad_norm': 0.6649631261825562, 'learning_rate': 4.335973137182458e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7466, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.0952805519104003, 'epoch': 0.3}
[2024-06-14 03:10:11,385][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (12, 0.0, {}, 22415.004694726085)
[2024-06-14 03:10:12,113][flwr][INFO] - fit progress: (12, 0.0, {}, 22415.004694726085)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 03:10:12,113][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 03:10:12,113][flwr][INFO] - 
[92mINFO [0m:      [ROUND 13]
[2024-06-14 03:10:12,113][flwr][INFO] - [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 03:10:12,113][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:17, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:03, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:50, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.60s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.60s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.29s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.31s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.69s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.57s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.68s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.31s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.32s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.48s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.68s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<02:04, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:15, 15.15s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:47, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.60s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.89s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:30, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.24s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2729.98 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2590.76 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:07, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.18s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1597, 'grad_norm': 0.5338401794433594, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9916, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.06, 'train_loss': 1.1596571922302246, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1014, 'grad_norm': 0.6235225796699524, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.0482, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1013755798339844, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2142, 'grad_norm': 0.6147562861442566, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.1878, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.2141883850097657, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1154, 'grad_norm': 0.6044722199440002, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.591, 'train_samples_per_second': 0.949, 'train_steps_per_second': 0.059, 'train_loss': 1.1153600692749024, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1014, 'grad_norm': 0.683290421962738, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.325, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.058, 'train_loss': 1.1014059066772461, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1756, 'grad_norm': 0.6172802448272705, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.7849, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.1756264686584472, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1357, 'grad_norm': 0.538137674331665, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.39, 'train_samples_per_second': 0.973, 'train_steps_per_second': 0.061, 'train_loss': 1.1357272148132325, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1536, 'grad_norm': 0.6174746751785278, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.8226, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.1536270141601563, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1262, 'grad_norm': 0.8929301500320435, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6967, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1261601448059082, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0947, 'grad_norm': 0.4946345388889313, 'learning_rate': 4.2271404095252874e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.8047, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.0946758270263672, 'epoch': 0.3}
[2024-06-14 03:41:44,263][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (13, 0.0, {}, 24307.887873637024)
[2024-06-14 03:41:44,996][flwr][INFO] - fit progress: (13, 0.0, {}, 24307.887873637024)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 03:41:44,996][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 03:41:44,996][flwr][INFO] - 
[92mINFO [0m:      [ROUND 14]
[2024-06-14 03:41:44,996][flwr][INFO] - [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 03:41:44,997][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.60s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2930.02 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2676.92 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.50s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:15, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:17, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:03, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:50, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.60s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.60s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.96s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:50, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.82s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.91s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:34, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.92s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:53, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:03, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.25s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.43s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.49s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2690.85 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2544.45 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:54, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:32, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.22s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.83s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1256, 'grad_norm': 0.5545614957809448, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2455, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1256139755249024, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.179, 'grad_norm': 0.4841150641441345, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.941, 'train_samples_per_second': 0.964, 'train_steps_per_second': 0.06, 'train_loss': 1.1790245056152344, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1541, 'grad_norm': 0.5374747514724731, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.0095, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.06, 'train_loss': 1.154096508026123, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1793, 'grad_norm': 0.48831647634506226, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.0978, 'train_samples_per_second': 1.025, 'train_steps_per_second': 0.064, 'train_loss': 1.17933931350708, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1094, 'grad_norm': 0.5312511920928955, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4272, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.1093955039978027, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1056, 'grad_norm': 0.5255464315414429, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.7035, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.058, 'train_loss': 1.105554962158203, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1015, 'grad_norm': 0.5863269567489624, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9279, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.101484489440918, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1214, 'grad_norm': 0.9252911806106567, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6897, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.121370792388916, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0758, 'grad_norm': 0.5414588451385498, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4226, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.075788402557373, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1385, 'grad_norm': 0.6625559329986572, 'learning_rate': 4.11168877488429e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0577, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1384862899780273, 'epoch': 0.3}
[2024-06-14 04:13:04,718][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (14, 0.0, {}, 26188.338318526046)
[2024-06-14 04:13:05,447][flwr][INFO] - fit progress: (14, 0.0, {}, 26188.338318526046)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 04:13:05,447][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 04:13:05,447][flwr][INFO] - 
[92mINFO [0m:      [ROUND 15]
[2024-06-14 04:13:05,447][flwr][INFO] - [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 04:13:05,447][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.00s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.44s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.23s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2862.22 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2713.39 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:34, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:09, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.51s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.76s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.09s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.80s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.04s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:56, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:42, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.61s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:43, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:18, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.81s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.37s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:55, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.69s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:37, 13.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:21, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:51, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.01s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1497, 'grad_norm': 0.5869831442832947, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.8267, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1496921539306642, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.178, 'grad_norm': 0.5227538347244263, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.8071, 'train_samples_per_second': 0.965, 'train_steps_per_second': 0.06, 'train_loss': 1.1780329704284669, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1321, 'grad_norm': 0.5275816321372986, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.6009, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.13209228515625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2076, 'grad_norm': 0.6316280961036682, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.184, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.2076400756835937, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1127, 'grad_norm': 0.5841927528381348, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9574, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.1127443313598633, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.137, 'grad_norm': 0.5195884704589844, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.8006, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1369901657104493, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1848, 'grad_norm': 0.7668026089668274, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.4878, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.056, 'train_loss': 1.1848295211791993, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1163, 'grad_norm': 0.5564699172973633, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.287, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1162895202636718, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0556, 'grad_norm': 0.48562347888946533, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.742, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.0555648803710938, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2, 'grad_norm': 0.7533836364746094, 'learning_rate': 3.99007386811656e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.1476, 'train_samples_per_second': 0.999, 'train_steps_per_second': 0.062, 'train_loss': 1.1999656677246093, 'epoch': 0.3}
[2024-06-14 04:44:33,641][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
[92mINFO [0m:      fit progress: (15, 0.0, {}, 28096.524380500196)
[2024-06-14 04:44:53,633][flwr][INFO] - fit progress: (15, 0.0, {}, 28096.524380500196)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 04:44:53,633][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 04:44:53,633][flwr][INFO] - 
[92mINFO [0m:      [ROUND 16]
[2024-06-14 04:44:53,633][flwr][INFO] - [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 04:44:53,634][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.35s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.08s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.13s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:24, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:43, 14.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:37, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:16, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<01:01, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:47, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.55s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.06s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.53s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m Using the latest cached version of the dataset since vicgalle/alpaca-gpt4 couldn't be found on the Hugging Face Hub
[36m(ClientAppActor pid=979026)[0m Found the latest cached dataset configuration 'default' at /home/tmehboob_umass_edu/.cache/huggingface/datasets/vicgalle___alpaca-gpt4/default/0.0.0/f7e3ded725cb81e8e564e32feb12860f376f2b51 (last modified on Fri Jun 14 04:52:07 2024).
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.24s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.50s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/521 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 521/521 [00:00<00:00, 2668.87 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 521/521 [00:00<00:00, 2526.01 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:00, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:43, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:30, 15.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:21, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:50, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.09s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.52s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.67s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.00s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.25s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.43s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:03, 13.68s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:11, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<02:04, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:15, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:47, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:33, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.86s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2757.42 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2597.20 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:50, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.45s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.20s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.54s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2769.60 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2598.31 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1154, 'grad_norm': 0.4872318506240845, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6432, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.1153680801391601, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1247, 'grad_norm': 0.6269369721412659, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0181, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1247052192687987, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1196, 'grad_norm': 0.7521626353263855, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.4065, 'train_samples_per_second': 1.016, 'train_steps_per_second': 0.064, 'train_loss': 1.1195672035217286, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1106, 'grad_norm': 0.9332238435745239, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.6978, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.110589599609375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1633, 'grad_norm': 0.5072957277297974, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9169, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1632754325866699, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1792, 'grad_norm': 0.5508519411087036, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0358, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.1791568756103517, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1275, 'grad_norm': 0.539299488067627, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.7379, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.061, 'train_loss': 1.1275091171264648, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1286, 'grad_norm': 0.5687010884284973, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.3789, 'train_samples_per_second': 0.973, 'train_steps_per_second': 0.061, 'train_loss': 1.1285540580749511, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1005, 'grad_norm': 0.5356112122535706, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.7773, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.100464153289795, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1703, 'grad_norm': 0.5741366147994995, 'learning_rate': 3.862775647698542e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.2149, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.058, 'train_loss': 1.170273780822754, 'epoch': 0.3}
[2024-06-14 05:15:33,921][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (16, 0.0, {}, 29937.544137608027)
[2024-06-14 05:15:34,652][flwr][INFO] - fit progress: (16, 0.0, {}, 29937.544137608027)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 05:15:34,653][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 05:15:34,653][flwr][INFO] - 
[92mINFO [0m:      [ROUND 17]
[2024-06-14 05:15:34,653][flwr][INFO] - [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 05:15:34,653][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2785.25 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2633.84 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.81s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.62s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:22, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:08, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:17, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:16, 15.12s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:46, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:30, 18.04s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.18s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:05<00:54, 18.27s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:23<00:36, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:18, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.13s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/521 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 521/521 [00:00<00:00, 2889.32 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 521/521 [00:00<00:00, 2670.40 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:55, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:03, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:31, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:14, 14.75s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.55s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.55s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.31s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:54, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:32, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2755.78 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2598.11 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:59, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:29, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.17s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1497, 'grad_norm': 0.5883777141571045, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.2241, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.149692440032959, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1316, 'grad_norm': 0.5934865474700928, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7353, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.131564235687256, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1144, 'grad_norm': 0.5513091087341309, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.3489, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.1144387245178222, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1198, 'grad_norm': 0.6317476034164429, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0723, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.062, 'train_loss': 1.1198365211486816, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1273, 'grad_norm': 0.48015928268432617, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.987, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.063, 'train_loss': 1.127332878112793, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1059, 'grad_norm': 0.9534385800361633, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.6965, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.1059492111206055, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2017, 'grad_norm': 0.6477225422859192, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.2021, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.2016714096069336, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0685, 'grad_norm': 0.5598338842391968, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4609, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.0685290336608886, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1467, 'grad_norm': 0.7091520428657532, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.8534, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.1467132568359375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1332, 'grad_norm': 0.7410797476768494, 'learning_rate': 3.7302965015492025e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7413, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1331552505493163, 'epoch': 0.3}
[2024-06-14 05:46:56,253][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (17, 0.0, {}, 31819.8388831201)
[2024-06-14 05:46:56,947][flwr][INFO] - fit progress: (17, 0.0, {}, 31819.8388831201)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 05:46:56,947][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 05:46:56,947][flwr][INFO] - 
[92mINFO [0m:      [ROUND 18]
[2024-06-14 05:46:56,947][flwr][INFO] - [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 05:46:56,948][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.90s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.08s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.31s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:36, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.50s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.15s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.15s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.15s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.18s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:53, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:00, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:18<00:15, 15.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.69s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2823.01 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2592.17 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.03s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:49, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:50, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.12s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.52s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:41, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:27, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:09, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:29, 14.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:17<00:13, 13.49s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 15.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:55, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.48s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:13, 16.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:02, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:47, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.94s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.13s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:15, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.59s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1137, 'grad_norm': 0.9644032120704651, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.694, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1136531829833984, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1654, 'grad_norm': 0.6845173239707947, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.8473, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.1653545379638672, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1641, 'grad_norm': 0.5944750905036926, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.2066, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.058, 'train_loss': 1.1641470909118652, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1265, 'grad_norm': 0.9644320607185364, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.4266, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.066, 'train_loss': 1.1264658927917481, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1574, 'grad_norm': 0.5954738855361938, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3705, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.062, 'train_loss': 1.1573858261108398, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1277, 'grad_norm': 0.7986140251159668, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.6823, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.127691650390625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1255, 'grad_norm': 0.5890727043151855, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 153.9529, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.065, 'train_loss': 1.1255373001098632, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0498, 'grad_norm': 0.5089216232299805, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7475, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.0497827529907227, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1314, 'grad_norm': 0.5891242027282715, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.031, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.058, 'train_loss': 1.1314237594604493, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1704, 'grad_norm': 0.5331376791000366, 'learning_rate': 3.593159264334428e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.9269, 'train_samples_per_second': 0.964, 'train_steps_per_second': 0.06, 'train_loss': 1.17036714553833, 'epoch': 0.3}
[2024-06-14 06:18:01,110][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (18, 0.0, {}, 33684.72591397818)
[2024-06-14 06:18:01,834][flwr][INFO] - fit progress: (18, 0.0, {}, 33684.72591397818)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 06:18:01,834][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 06:18:01,834][flwr][INFO] - 
[92mINFO [0m:      [ROUND 19]
[2024-06-14 06:18:01,834][flwr][INFO] - [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 06:18:01,835][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:50, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.10s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:43, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:28, 14.70s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3496.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3264.24 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:09, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2727.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2510.80 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:19, 15.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:56, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.03s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.54s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3500.51 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 3189.20 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:23, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.18s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2600.02 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2481.34 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.78s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.27s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.50s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.91s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2775.56 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2632.23 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:15, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:11, 14.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:49, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.14s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1696, 'grad_norm': 0.5363369584083557, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.0925, 'train_samples_per_second': 1.025, 'train_steps_per_second': 0.064, 'train_loss': 1.1695812225341797, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1237, 'grad_norm': 0.673479437828064, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.3402, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.06, 'train_loss': 1.1237115859985352, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1693, 'grad_norm': 0.7228145003318787, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.0349, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.1692893981933594, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1035, 'grad_norm': 0.663896381855011, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.603, 'train_samples_per_second': 0.949, 'train_steps_per_second': 0.059, 'train_loss': 1.1035300254821778, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1328, 'grad_norm': 0.7795646786689758, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.4338, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'train_loss': 1.1328149795532227, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.2077, 'grad_norm': 0.6529908776283264, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0046, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.2077234268188477, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1225, 'grad_norm': 0.6341001391410828, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.1292, 'train_samples_per_second': 0.919, 'train_steps_per_second': 0.057, 'train_loss': 1.1224784851074219, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.19, 'grad_norm': 0.8366113901138306, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.7541, 'train_samples_per_second': 1.021, 'train_steps_per_second': 0.064, 'train_loss': 1.1899730682373046, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1498, 'grad_norm': 0.7612336277961731, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.5186, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.1498234748840332, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1512, 'grad_norm': 0.7443565130233765, 'learning_rate': 3.451905154077462e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.4493, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'train_loss': 1.1512120246887207, 'epoch': 0.3}
[2024-06-14 06:49:02,762][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (19, 0.0, {}, 35546.36807923415)
[2024-06-14 06:49:03,476][flwr][INFO] - fit progress: (19, 0.0, {}, 35546.36807923415)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 06:49:03,477][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 06:49:03,477][flwr][INFO] - 
[92mINFO [0m:      [ROUND 20]
[2024-06-14 06:49:03,477][flwr][INFO] - [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 06:49:03,477][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:41, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:27, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:09, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:29, 14.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:17<00:13, 13.49s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 14.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 15.39s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.14s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2510.24 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2381.98 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:04, 15.52s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:47, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:18, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:06, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:51, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.20s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.55s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:04, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:45, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:29, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:12, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:54, 18.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:36, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.78s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:16, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:57, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:55, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.17s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.98s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:55, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:46, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:30, 18.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.17s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:05<00:54, 18.27s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:23<00:36, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:18, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.53s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1207, 'grad_norm': 0.6203548312187195, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 153.9399, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.065, 'train_loss': 1.1206527709960938, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1088, 'grad_norm': 0.9863342046737671, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.7082, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1088226318359375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0886, 'grad_norm': 0.6072831153869629, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.7502, 'train_samples_per_second': 0.954, 'train_steps_per_second': 0.06, 'train_loss': 1.0886168479919434, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1501, 'grad_norm': 0.592406690120697, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.034, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.057, 'train_loss': 1.1500917434692384, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1755, 'grad_norm': 0.7598147988319397, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.7542, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.1755206108093261, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0718, 'grad_norm': 0.6268141269683838, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.5617, 'train_samples_per_second': 1.009, 'train_steps_per_second': 0.063, 'train_loss': 1.0718421936035156, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1159, 'grad_norm': 0.6480721235275269, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9101, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1159189224243165, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1276, 'grad_norm': 0.5605581998825073, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.8316, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1275850296020509, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1286, 'grad_norm': 0.794810950756073, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.4092, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'train_loss': 1.128622817993164, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1085, 'grad_norm': 0.5825986862182617, 'learning_rate': 3.307091636218621e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.2686, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.057, 'train_loss': 1.1084859848022461, 'epoch': 0.3}
[2024-06-14 07:20:01,207][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[92mINFO [0m:      fit progress: (20, 0.0, {}, 37424.35191479721)
[2024-06-14 07:20:21,460][flwr][INFO] - fit progress: (20, 0.0, {}, 37424.35191479721)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 07:20:21,461][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 07:20:21,461][flwr][INFO] - 
[92mINFO [0m:      [ROUND 21]
[2024-06-14 07:20:21,461][flwr][INFO] - [ROUND 21]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 07:20:21,461][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.39s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.04s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:36, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:16, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.34s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.34s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.62s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2973.31 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2806.84 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:48, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:25, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:51, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.20s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2935.67 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2775.68 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:01, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:16, 15.36s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:05, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2701.26 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2562.90 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.74s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:47, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:28, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:34, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:17, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.16s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.52s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:24, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.31s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.50s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:57, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1153, 'grad_norm': 0.8182097673416138, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.1136, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.115279769897461, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0887, 'grad_norm': 0.6805226802825928, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.0688, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.0887201309204102, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1538, 'grad_norm': 0.6133427023887634, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.6656, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1537915229797364, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1577, 'grad_norm': 0.6213206648826599, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.1983, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.058, 'train_loss': 1.1576539993286132, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1162, 'grad_norm': 0.6682167053222656, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7544, 'train_samples_per_second': 0.959, 'train_steps_per_second': 0.06, 'train_loss': 1.1162101745605468, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.128, 'grad_norm': 0.6542659401893616, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.9628, 'train_samples_per_second': 0.909, 'train_steps_per_second': 0.057, 'train_loss': 1.1279729843139648, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0885, 'grad_norm': 0.7616591453552246, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.3422, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.058, 'train_loss': 1.08851318359375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0665, 'grad_norm': 0.7758468389511108, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9435, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.0665043830871581, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1032, 'grad_norm': 0.6348010897636414, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9804, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.06, 'train_loss': 1.1031658172607421, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.112, 'grad_norm': 0.6719521284103394, 'learning_rate': 3.159290223553894e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9257, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1120466232299804, 'epoch': 0.3}
[2024-06-14 07:52:02,518][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (21, 0.0, {}, 39326.10217083013)
[2024-06-14 07:52:03,210][flwr][INFO] - fit progress: (21, 0.0, {}, 39326.10217083013)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 07:52:03,211][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 07:52:03,211][flwr][INFO] - 
[92mINFO [0m:      [ROUND 22]
[2024-06-14 07:52:03,211][flwr][INFO] - [ROUND 22]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 07:52:03,211][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:23, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.79s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.53s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.95s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.45s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.23s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.98s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.99s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.10s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:43, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:28, 14.70s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:54, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:32, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2602.71 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2479.90 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:15, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:47, 17.94s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.20s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1167, 'grad_norm': 0.652138352394104, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.1482, 'train_samples_per_second': 0.919, 'train_steps_per_second': 0.057, 'train_loss': 1.1166754722595216, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0978, 'grad_norm': 0.9720371961593628, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.7249, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.0978460311889648, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1449, 'grad_norm': 0.6038857102394104, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6974, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.1448646545410157, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1679, 'grad_norm': 0.5854480862617493, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4564, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1678548812866212, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1504, 'grad_norm': 0.6194596290588379, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3494, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1504158973693848, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1188, 'grad_norm': 0.588607907295227, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.6165, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.1187580108642579, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.118, 'grad_norm': 0.7089769840240479, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.304, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.06, 'train_loss': 1.1179978370666503, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0612, 'grad_norm': 0.6726118922233582, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.431, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.0611540794372558, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.085, 'grad_norm': 0.7732174396514893, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.3268, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.058, 'train_loss': 1.0849717140197754, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1747, 'grad_norm': 0.6304168701171875, 'learning_rate': 3.0090842207350262e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.9517, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.1747272491455079, 'epoch': 0.3}
[2024-06-14 08:23:21,694][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (22, 0.0, {}, 41205.30550027918)
[2024-06-14 08:23:22,414][flwr][INFO] - fit progress: (22, 0.0, {}, 41205.30550027918)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 08:23:22,414][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 08:23:22,414][flwr][INFO] - 
[92mINFO [0m:      [ROUND 23]
[2024-06-14 08:23:22,414][flwr][INFO] - [ROUND 23]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 08:23:22,414][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.53s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.61s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.85s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:04, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:45, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:29, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:12, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:54, 18.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:36, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.78s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2783.84 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2570.17 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:10, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:50, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.94s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2833.88 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2681.04 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:33, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:31, 18.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:52, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.16s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.20s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:53, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:00, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:18<00:15, 15.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:55, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:03, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:31, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:14, 14.75s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.94s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:48, 18.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:25, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:51, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.13s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.78s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1727, 'grad_norm': 0.8368304967880249, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.5136, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.056, 'train_loss': 1.1726704597473145, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.145, 'grad_norm': 0.6066449880599976, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.0363, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.057, 'train_loss': 1.1449564933776855, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1494, 'grad_norm': 0.6851869821548462, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.6244, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.149429702758789, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0914, 'grad_norm': 0.6044288873672485, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.7647, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.0914344787597656, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0915, 'grad_norm': 0.7005055546760559, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.9321, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.059, 'train_loss': 1.0915014266967773, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0943, 'grad_norm': 0.9937835931777954, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.727, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.094282341003418, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1189, 'grad_norm': 1.0262408256530762, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.4286, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.066, 'train_loss': 1.1188730239868163, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1197, 'grad_norm': 0.5246978402137756, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.9836, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.063, 'train_loss': 1.119738483428955, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1494, 'grad_norm': 0.6250993609428406, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.7049, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1493865966796875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1833, 'grad_norm': 0.8721027970314026, 'learning_rate': 2.8570664222325463e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.7594, 'train_samples_per_second': 1.021, 'train_steps_per_second': 0.064, 'train_loss': 1.1833239555358888, 'epoch': 0.3}
[2024-06-14 08:54:42,379][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (23, 0.0, {}, 43085.943228806136)
[2024-06-14 08:54:43,051][flwr][INFO] - fit progress: (23, 0.0, {}, 43085.943228806136)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 08:54:43,052][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 08:54:43,052][flwr][INFO] - 
[92mINFO [0m:      [ROUND 24]
[2024-06-14 08:54:43,052][flwr][INFO] - [ROUND 24]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 08:54:43,052][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:19, 15.55s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.14s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.69s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.67s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.25s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.43s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.17s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:47, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:28, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:34, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:17, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.85s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:16, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.37s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:52, 14.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.19s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.55s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.05s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2693.05 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2552.15 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.39s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:58, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.94s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.11s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1445, 'grad_norm': 0.7665793895721436, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.417, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1444967269897461, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0784, 'grad_norm': 0.5589712858200073, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.7862, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.0784024238586425, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.103, 'grad_norm': 1.0147514343261719, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.7279, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1030340194702148, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1679, 'grad_norm': 0.6080312132835388, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0364, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.167877197265625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1235, 'grad_norm': 0.6754904389381409, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.9605, 'train_samples_per_second': 0.909, 'train_steps_per_second': 0.057, 'train_loss': 1.1234780311584474, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1526, 'grad_norm': 0.6492545008659363, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.2279, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.058, 'train_loss': 1.1525675773620605, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0653, 'grad_norm': 0.6604613661766052, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.5758, 'train_samples_per_second': 1.009, 'train_steps_per_second': 0.063, 'train_loss': 1.0652626991271972, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.111, 'grad_norm': 0.6556280851364136, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0099, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1109981536865234, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1377, 'grad_norm': 0.6548556685447693, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.8416, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1377449989318849, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1724, 'grad_norm': 0.6272653341293335, 'learning_rate': 2.7038367728468184e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.1175, 'train_samples_per_second': 0.993, 'train_steps_per_second': 0.062, 'train_loss': 1.1724232673645019, 'epoch': 0.3}
[2024-06-14 09:25:53,897][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (24, 0.0, {}, 44957.52103651711)
[2024-06-14 09:25:54,629][flwr][INFO] - fit progress: (24, 0.0, {}, 44957.52103651711)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 09:25:54,629][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 09:25:54,630][flwr][INFO] - 
[92mINFO [0m:      [ROUND 25]
[2024-06-14 09:25:54,630][flwr][INFO] - [ROUND 25]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 09:25:54,630][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.01s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:33, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:31, 18.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:52, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.89s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.95s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2778.13 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2630.73 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.64s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:57, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.44s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.72s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:24, 18.02s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:06, 14.01s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:02, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:47, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.94s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.43s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.61s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.85s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.37s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.69s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:09, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:49, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.92s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.61s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:58, 14.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.17s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0873, 'grad_norm': 0.7234641909599304, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.9556, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.058, 'train_loss': 1.0873249053955079, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0836, 'grad_norm': 0.7121108770370483, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.0792, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.0836153984069825, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1263, 'grad_norm': 0.5745211839675903, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.0797, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1263100624084472, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.107, 'grad_norm': 0.7175921201705933, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9542, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1070476531982423, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0611, 'grad_norm': 0.8303022980690002, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9771, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.0610968589782714, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1215, 'grad_norm': 0.6536531448364258, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.0561, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.058, 'train_loss': 1.1215252876281738, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1239, 'grad_norm': 0.6938347220420837, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.1164, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1238500595092773, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1234, 'grad_norm': 0.5904912948608398, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.8424, 'train_samples_per_second': 0.959, 'train_steps_per_second': 0.06, 'train_loss': 1.123423957824707, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1042, 'grad_norm': 0.5473362803459167, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6577, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.1041563987731933, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.139, 'grad_norm': 0.656970739364624, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.1976, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.1390074729919433, 'epoch': 0.3}
[2024-06-14 09:57:15,978][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[92mINFO [0m:      fit progress: (25, 0.0, {}, 46858.551739855204)
[2024-06-14 09:57:35,660][flwr][INFO] - fit progress: (25, 0.0, {}, 46858.551739855204)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 09:57:35,660][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 09:57:35,661][flwr][INFO] - 
[92mINFO [0m:      [ROUND 26]
[2024-06-14 09:57:35,661][flwr][INFO] - [ROUND 26]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 09:57:35,661][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.08s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.96s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.39s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.39s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2787.77 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2633.13 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:49, 12.16s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:06, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:57, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:22, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:52, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.24s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.18s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:53, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:00, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:18<00:15, 15.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.80s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.55s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.92s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:43, 14.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.65s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.70s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:56, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:54, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:36, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.41s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.89s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.12s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.12s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.12s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1393, 'grad_norm': 0.632441520690918, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.7188, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.1392765045166016, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1596, 'grad_norm': 0.6366235017776489, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.214, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.1595805168151856, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1094, 'grad_norm': 0.6405096054077148, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.0954, 'train_samples_per_second': 0.981, 'train_steps_per_second': 0.061, 'train_loss': 1.1094200134277343, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1144, 'grad_norm': 1.0480023622512817, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.4244, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.066, 'train_loss': 1.114421272277832, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1556, 'grad_norm': 0.7583180665969849, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.8076, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.1555526733398438, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1009, 'grad_norm': 0.561091423034668, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.647, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.1008648872375488, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.16, 'grad_norm': 0.8724663853645325, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.3643, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.062, 'train_loss': 1.1599746704101563, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0969, 'grad_norm': 0.6556209921836853, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9783, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.0968947410583496, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1323, 'grad_norm': 0.7867982983589172, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.48, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.132339859008789, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1233, 'grad_norm': 0.5825565457344055, 'learning_rate': 2.3961632271531826e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.1447, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1233081817626953, 'epoch': 0.3}
[2024-06-14 10:28:39,859][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (26, 0.0, {}, 48723.442336865)
[2024-06-14 10:28:40,551][flwr][INFO] - fit progress: (26, 0.0, {}, 48723.442336865)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 10:28:40,551][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 10:28:40,551][flwr][INFO] - 
[92mINFO [0m:      [ROUND 27]
[2024-06-14 10:28:40,551][flwr][INFO] - [ROUND 27]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 10:28:40,551][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.16s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.70s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:16, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.16s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.82s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:16, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.21s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.54s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.12s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.05s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:56, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:42, 14.58s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.22s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.78s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:34, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:09, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.60s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:43, 14.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:28, 14.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:07, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.16s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:43, 11.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<02:04, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:15, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:47, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:26, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1207, 'grad_norm': 0.5923734307289124, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.0884, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1207191467285156, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0608, 'grad_norm': 0.6666792035102844, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.5623, 'train_samples_per_second': 1.009, 'train_steps_per_second': 0.063, 'train_loss': 1.060837459564209, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1061, 'grad_norm': 0.5350809097290039, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9991, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.1060843467712402, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.144, 'grad_norm': 0.6144600510597229, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.341, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1439614295959473, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1195, 'grad_norm': 0.5715671181678772, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.8176, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1194839477539062, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1624, 'grad_norm': 0.5777902603149414, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.8007, 'train_samples_per_second': 0.965, 'train_steps_per_second': 0.06, 'train_loss': 1.1624224662780762, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1124, 'grad_norm': 0.7505148649215698, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.3293, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.06, 'train_loss': 1.112373447418213, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1152, 'grad_norm': 0.6081315279006958, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.3629, 'train_samples_per_second': 0.973, 'train_steps_per_second': 0.061, 'train_loss': 1.115247917175293, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1985, 'grad_norm': 0.7320963740348816, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0066, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1984989166259765, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1109, 'grad_norm': 0.732083261013031, 'learning_rate': 2.242933577767455e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1847, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1108545303344726, 'epoch': 0.3}
[2024-06-14 10:59:25,058][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (27, 0.0, {}, 50568.62469129008)
[2024-06-14 10:59:25,733][flwr][INFO] - fit progress: (27, 0.0, {}, 50568.62469129008)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 10:59:25,733][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 10:59:25,733][flwr][INFO] - 
[92mINFO [0m:      [ROUND 28]
[2024-06-14 10:59:25,733][flwr][INFO] - [ROUND 28]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 10:59:25,733][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.78s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.83s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:54, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:32, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.70s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.96s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:26, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:09, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.31s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.76s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:07, 14.19s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:53, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:00, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:18<00:15, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 13.95s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.04s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.65s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.17s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:47, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:28, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:34, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:17, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.44s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:09<01:27,  9.78s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:24, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.95s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0551, 'grad_norm': 0.6768595576286316, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.449, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.0551225662231445, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1608, 'grad_norm': 0.6154674887657166, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4483, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.160759162902832, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.108, 'grad_norm': 0.7486676573753357, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.196, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1079584121704102, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1523, 'grad_norm': 0.7804471850395203, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.7942, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.1523269653320312, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1593, 'grad_norm': 0.7569209337234497, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.0095, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.159279727935791, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.133, 'grad_norm': 0.6780827045440674, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.8345, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1329648971557618, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1112, 'grad_norm': 1.1010860204696655, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 150.4284, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.066, 'train_loss': 1.1112040519714355, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0987, 'grad_norm': 1.0409648418426514, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6922, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.0986663818359375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1192, 'grad_norm': 0.6716625690460205, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.9911, 'train_samples_per_second': 0.909, 'train_steps_per_second': 0.057, 'train_loss': 1.1192187309265136, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1414, 'grad_norm': 0.6459063291549683, 'learning_rate': 2.090915779264975e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.5308, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'train_loss': 1.141389751434326, 'epoch': 0.3}
[2024-06-14 11:30:35,263][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (28, 0.0, {}, 52438.86346018803)
[2024-06-14 11:30:35,972][flwr][INFO] - fit progress: (28, 0.0, {}, 52438.86346018803)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 11:30:35,972][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 11:30:35,972][flwr][INFO] - 
[92mINFO [0m:      [ROUND 29]
[2024-06-14 11:30:35,972][flwr][INFO] - [ROUND 29]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 11:30:35,972][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.72s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.50s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.39s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.06s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.41s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:58, 13.17s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:51, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:24, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:50, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:48, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:25, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:06, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:51, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:36, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:24, 18.02s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 15.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.09s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:07, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:39, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:22, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:00, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:16, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.089, 'grad_norm': 1.0254019498825073, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.6973, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'train_loss': 1.0890397071838378, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1342, 'grad_norm': 0.6848250031471252, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.2053, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.134227180480957, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1409, 'grad_norm': 0.6201152205467224, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3179, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1408896446228027, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1345, 'grad_norm': 0.8025227189064026, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8039, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1345290184020995, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1439, 'grad_norm': 0.6531708836555481, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.6981, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1439002990722655, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0571, 'grad_norm': 0.8536895513534546, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9919, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.0571425437927247, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1952, 'grad_norm': 0.752453088760376, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0103, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1951621055603028, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0867, 'grad_norm': 0.6684452891349792, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8794, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.0866912841796874, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.103, 'grad_norm': 0.5418420433998108, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9879, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.1029902458190919, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1886, 'grad_norm': 0.7148692607879639, 'learning_rate': 1.940709776446106e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.2049, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.1885830879211425, 'epoch': 0.3}
[2024-06-14 12:01:39,032][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (29, 0.0, {}, 54302.614620501176)
[2024-06-14 12:01:39,723][flwr][INFO] - fit progress: (29, 0.0, {}, 54302.614620501176)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 12:01:39,723][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 12:01:39,723][flwr][INFO] - 
[92mINFO [0m:      [ROUND 30]
[2024-06-14 12:01:39,723][flwr][INFO] - [ROUND 30]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 12:01:39,723][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.86s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:26, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:44, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m Using the latest cached version of the dataset since vicgalle/alpaca-gpt4 couldn't be found on the Hugging Face Hub
[36m(ClientAppActor pid=979026)[0m Found the latest cached dataset configuration 'default' at /home/tmehboob_umass_edu/.cache/huggingface/datasets/vicgalle___alpaca-gpt4/default/0.0.0/f7e3ded725cb81e8e564e32feb12860f376f2b51 (last modified on Fri Jun 14 11:58:35 2024).
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.62s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.10s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.12s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:33, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:23, 17.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.27s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:31, 18.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:52, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:50, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.71s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:54, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:36, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:55, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:42, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.50s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.62s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.62s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.17s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1165, 'grad_norm': 0.6941431760787964, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.0371, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.063, 'train_loss': 1.1165498733520507, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1576, 'grad_norm': 0.6277913451194763, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4534, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1575759887695312, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0933, 'grad_norm': 0.6420243382453918, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4129, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.0933133125305177, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0827, 'grad_norm': 0.7722711563110352, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.9414, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.058, 'train_loss': 1.0826760292053224, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1377, 'grad_norm': 0.9069609045982361, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.5254, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.137723159790039, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1118, 'grad_norm': 0.630776584148407, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.6009, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.1118135452270508, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1165, 'grad_norm': 0.9091514348983765, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.648, 'train_samples_per_second': 0.938, 'train_steps_per_second': 0.059, 'train_loss': 1.1165094375610352, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.128, 'grad_norm': 0.8178391456604004, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.4968, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.1280385971069335, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1155, 'grad_norm': 0.5726171731948853, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.7884, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1154870986938477, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0971, 'grad_norm': 0.56941157579422, 'learning_rate': 1.7929083637813798e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6533, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.097126293182373, 'epoch': 0.3}
[2024-06-14 12:32:55,308][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[92mINFO [0m:      fit progress: (30, 0.0, {}, 56197.7227400681)
[2024-06-14 12:33:14,831][flwr][INFO] - fit progress: (30, 0.0, {}, 56197.7227400681)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 12:33:14,831][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 12:33:14,831][flwr][INFO] - 
[92mINFO [0m:      [ROUND 31]
[2024-06-14 12:33:14,831][flwr][INFO] - [ROUND 31]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 12:33:14,831][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.57s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.70s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:37, 13.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:21, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:51, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 15.96s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:40<00:00, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.67s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:40, 14.33s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:18, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:00, 15.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:09, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:27, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.14s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.17s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.53s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:54, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:32, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.88s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:50, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.45s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1845, 'grad_norm': 0.9067217111587524, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 160.1714, 'train_samples_per_second': 0.999, 'train_steps_per_second': 0.062, 'train_loss': 1.1845366477966308, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.166, 'grad_norm': 0.8587230443954468, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.7578, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.1660210609436035, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0741, 'grad_norm': 0.8181074857711792, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.3001, 'train_samples_per_second': 1.011, 'train_steps_per_second': 0.063, 'train_loss': 1.0740772247314454, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1559, 'grad_norm': 0.7651665210723877, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.0084, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.1559246063232422, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1048, 'grad_norm': 0.763048529624939, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1578, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1048291206359864, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1177, 'grad_norm': 0.6153966784477234, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.0747, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1177240371704102, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1314, 'grad_norm': 0.6958001255989075, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.2311, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.1313962936401367, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1218, 'grad_norm': 0.8624640703201294, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7076, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.121784496307373, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0522, 'grad_norm': 0.6805241107940674, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4117, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.0521778106689452, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0853, 'grad_norm': 0.6815504431724548, 'learning_rate': 1.64809484592254e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.7749, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.0852843284606934, 'epoch': 0.3}
[2024-06-14 13:04:35,782][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (31, 0.0, {}, 58079.3598929171)
[2024-06-14 13:04:36,468][flwr][INFO] - fit progress: (31, 0.0, {}, 58079.3598929171)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 13:04:36,468][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 13:04:36,468][flwr][INFO] - 
[92mINFO [0m:      [ROUND 32]
[2024-06-14 13:04:36,468][flwr][INFO] - [ROUND 32]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 13:04:36,469][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.21s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.14s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.56s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.09s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:04, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2568.76 examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2249.96 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:18, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:41, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.22s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.18s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.96s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.80s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:15, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.14s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.50s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.155, 'grad_norm': 0.6309089064598083, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4677, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1549598693847656, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1291, 'grad_norm': 0.6969461441040039, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.2249, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.1290692329406737, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.091, 'grad_norm': 0.6509709358215332, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.4071, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.059, 'train_loss': 1.091015911102295, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1027, 'grad_norm': 0.7720521092414856, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1602, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1027145385742188, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1107, 'grad_norm': 0.6289536952972412, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.2925, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1107000350952148, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1094, 'grad_norm': 0.6487356424331665, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.6479, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.1094144821166991, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0786, 'grad_norm': 0.7728642225265503, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.0583, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.0786262512207032, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1344, 'grad_norm': 0.6657289862632751, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.7169, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.134434700012207, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1001, 'grad_norm': 0.5649716854095459, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9825, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.1000613212585448, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0953, 'grad_norm': 1.0619369745254517, 'learning_rate': 1.5068407356655722e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6906, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.0953001976013184, 'epoch': 0.3}
[2024-06-14 13:35:35,249][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (32, 0.0, {}, 59938.85760164005)
[2024-06-14 13:35:35,966][flwr][INFO] - fit progress: (32, 0.0, {}, 59938.85760164005)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 13:35:35,966][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 13:35:35,966][flwr][INFO] - 
[92mINFO [0m:      [ROUND 33]
[2024-06-14 13:35:35,966][flwr][INFO] - [ROUND 33]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 13:35:35,966][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.41s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.06s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.86s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.78s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.17s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:08, 18.33s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:06, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:43, 14.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:28, 14.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:20, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:52, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m                                                100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.09s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:40, 14.32s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:18, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:00, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.96s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.37s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:00, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:43, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:30, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:21, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:50, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:03, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.56s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.08s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1755, 'grad_norm': 0.9437443614006042, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.7943, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.175503635406494, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1342, 'grad_norm': 0.6326110363006592, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.1496, 'train_samples_per_second': 0.929, 'train_steps_per_second': 0.058, 'train_loss': 1.1342039108276367, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1081, 'grad_norm': 0.7699646949768066, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.3097, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.06, 'train_loss': 1.1081055641174316, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.072, 'grad_norm': 0.8189265131950378, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.2763, 'train_samples_per_second': 1.011, 'train_steps_per_second': 0.063, 'train_loss': 1.0719672203063966, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.153, 'grad_norm': 0.6379634141921997, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.455, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1529735565185546, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1478, 'grad_norm': 0.6154600381851196, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9325, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1478140830993653, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.164, 'grad_norm': 0.8996406197547913, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.557, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.056, 'train_loss': 1.163961410522461, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0837, 'grad_norm': 0.7731295824050903, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9317, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.0837387084960937, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1195, 'grad_norm': 0.8738638162612915, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7093, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1195060729980468, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1291, 'grad_norm': 0.6974645256996155, 'learning_rate': 1.3697034984507974e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.8495, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.059, 'train_loss': 1.1291443824768066, 'epoch': 0.3}
[2024-06-14 14:06:40,545][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (33, 0.0, {}, 61804.16159589519)
[2024-06-14 14:06:41,270][flwr][INFO] - fit progress: (33, 0.0, {}, 61804.16159589519)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 14:06:41,270][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 14:06:41,270][flwr][INFO] - 
[92mINFO [0m:      [ROUND 34]
[2024-06-14 14:06:41,270][flwr][INFO] - [ROUND 34]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 14:06:41,270][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.51s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:43, 14.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:37, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:16, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<01:01, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:02, 13.56s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:38, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:23, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.55s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.79s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.29s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.44s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.96s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:01, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:46, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.42s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:50, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.22s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.91s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.52s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.85s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.72s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:50, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:48, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:25, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:51, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.69s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:49, 12.19s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:07, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:57, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:22, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:52, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.31s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.103, 'grad_norm': 0.9204666018486023, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.4433, 'train_samples_per_second': 1.016, 'train_steps_per_second': 0.064, 'train_loss': 1.1029998779296875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1075, 'grad_norm': 0.6542422771453857, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.5929, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'train_loss': 1.107509231567383, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1343, 'grad_norm': 0.9127301573753357, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.5165, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.057, 'train_loss': 1.1342854499816895, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1068, 'grad_norm': 0.717789888381958, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2546, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1067550659179688, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1039, 'grad_norm': 0.697716474533081, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6776, 'train_samples_per_second': 0.984, 'train_steps_per_second': 0.061, 'train_loss': 1.1038803100585937, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.075, 'grad_norm': 0.8085789084434509, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7202, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.0749516487121582, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0857, 'grad_norm': 1.0575710535049438, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.7868, 'train_samples_per_second': 1.001, 'train_steps_per_second': 0.063, 'train_loss': 1.0857163429260255, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1572, 'grad_norm': 0.6446466445922852, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.1382, 'train_samples_per_second': 1.025, 'train_steps_per_second': 0.064, 'train_loss': 1.1572040557861327, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1402, 'grad_norm': 0.6726164221763611, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.6544, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1401832580566407, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1036, 'grad_norm': 0.6678506135940552, 'learning_rate': 1.2372243523014579e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.1294, 'train_samples_per_second': 0.981, 'train_steps_per_second': 0.061, 'train_loss': 1.1036366462707519, 'epoch': 0.3}
[2024-06-14 14:37:36,764][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (34, 0.0, {}, 63660.31887688814)
[2024-06-14 14:37:37,427][flwr][INFO] - fit progress: (34, 0.0, {}, 63660.31887688814)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 14:37:37,427][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 14:37:37,427][flwr][INFO] - 
[92mINFO [0m:      [ROUND 35]
[2024-06-14 14:37:37,427][flwr][INFO] - [ROUND 35]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 14:37:37,428][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.30s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.55s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:27, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:25, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:30, 15.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:40, 17.88s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.86s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:09, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 14.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:14, 14.84s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:43<00:42, 14.11s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:59<00:29, 14.73s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:13<00:14, 14.66s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:32<00:00, 15.24s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:57, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.57s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:40, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:12, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:00, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.37s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:44, 14.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:35, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1051, 'grad_norm': 0.7286558747291565, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2412, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1050826072692872, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1153, 'grad_norm': 0.6408253908157349, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.0614, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1153136253356934, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1415, 'grad_norm': 0.7333856821060181, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.0892, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.1415388107299804, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1634, 'grad_norm': 0.8914539217948914, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.7562, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.16337890625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.151, 'grad_norm': 0.6415507197380066, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.45, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1510332107543946, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1529, 'grad_norm': 0.7668507695198059, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.0262, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.152920913696289, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1657, 'grad_norm': 0.6554586887359619, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 152.3617, 'train_samples_per_second': 1.05, 'train_steps_per_second': 0.066, 'train_loss': 1.165717887878418, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1011, 'grad_norm': 0.7830114960670471, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9152, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1011438369750977, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1212, 'grad_norm': 0.6382765769958496, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.4103, 'train_samples_per_second': 0.956, 'train_steps_per_second': 0.06, 'train_loss': 1.1212085723876952, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1128, 'grad_norm': 0.8148073554039001, 'learning_rate': 1.1099261318834413e-05, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0259, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.1127766609191894, 'epoch': 0.3}
[2024-06-14 15:08:30,455][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]
[92mINFO [0m:      fit progress: (35, 0.0, {}, 65532.4508715251)
[2024-06-14 15:08:49,559][flwr][INFO] - fit progress: (35, 0.0, {}, 65532.4508715251)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 15:08:49,560][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 15:08:49,560][flwr][INFO] - 
[92mINFO [0m:      [ROUND 36]
[2024-06-14 15:08:49,560][flwr][INFO] - [ROUND 36]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 15:08:49,560][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.72s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:50, 15.84s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.77s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:46, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:30, 18.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.17s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:05<00:54, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:23<00:36, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:18, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 16.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.81s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:26, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.41s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.75s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.90s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:04, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:47, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:18, 15.62s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:06, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:51, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.39s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:58, 13.17s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:51, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:24, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:50, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.94s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:07, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.43s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:04, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:45, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:52, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.65s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:09, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:46, 15.65s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.76s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1554, 'grad_norm': 0.6511306166648865, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.119, 'train_samples_per_second': 1.025, 'train_steps_per_second': 0.064, 'train_loss': 1.1553924560546875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.098, 'grad_norm': 0.6888489127159119, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.2603, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.057, 'train_loss': 1.0980060577392579, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1006, 'grad_norm': 0.7720307111740112, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1799, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.1005878448486328, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1839, 'grad_norm': 0.7529744505882263, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.1798, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.1838818550109864, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0767, 'grad_norm': 0.698860228061676, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.7648, 'train_samples_per_second': 0.954, 'train_steps_per_second': 0.06, 'train_loss': 1.0767248153686524, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1297, 'grad_norm': 0.869251549243927, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.8277, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1297256469726562, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1163, 'grad_norm': 0.7446048259735107, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0972, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1162760734558106, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0708, 'grad_norm': 0.5817527174949646, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.7834, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.070753574371338, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1138, 'grad_norm': 0.6505171656608582, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.6415, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.057, 'train_loss': 1.1137907028198242, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1089, 'grad_norm': 0.6397743225097656, 'learning_rate': 9.883112251157103e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.5903, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.06, 'train_loss': 1.1088925361633302, 'epoch': 0.3}
[2024-06-14 15:40:18,197][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (36, 0.0, {}, 67421.8296098751)
[2024-06-14 15:40:18,938][flwr][INFO] - fit progress: (36, 0.0, {}, 67421.8296098751)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 15:40:18,938][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 15:40:18,938][flwr][INFO] - 
[92mINFO [0m:      [ROUND 37]
[2024-06-14 15:40:18,938][flwr][INFO] - [ROUND 37]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 15:40:18,938][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.17s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.55s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:43, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:18, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.31s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.09s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:10, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.38s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.57s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.70s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.44s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.44s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:40, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:12, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:00, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.34s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.34s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.07s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:27, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:02, 15.75s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.99s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.37s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.80s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.97s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:56, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:54, 18.07s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:36, 18.15s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.80s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:37<02:28, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:04, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:48, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:28, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:34, 17.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:17, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.61s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0988, 'grad_norm': 0.7138469815254211, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2838, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.098775577545166, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1573, 'grad_norm': 0.906075119972229, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.8332, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.061, 'train_loss': 1.1572643280029298, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1591, 'grad_norm': 0.6598526835441589, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0984, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.1590520858764648, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0993, 'grad_norm': 0.7708631157875061, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1773, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.0993228912353517, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1174, 'grad_norm': 0.8739883899688721, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7502, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1173571586608886, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1195, 'grad_norm': 0.6465693116188049, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.5135, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.06, 'train_loss': 1.1194636344909668, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1139, 'grad_norm': 0.6485753059387207, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.3247, 'train_samples_per_second': 0.974, 'train_steps_per_second': 0.061, 'train_loss': 1.1139015197753905, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.173, 'grad_norm': 0.9578295946121216, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.0891, 'train_samples_per_second': 1.019, 'train_steps_per_second': 0.064, 'train_loss': 1.1730106353759766, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1242, 'grad_norm': 0.8484432101249695, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.71, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1242144584655762, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1152, 'grad_norm': 0.6839540600776672, 'learning_rate': 8.728595904747127e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 176.0966, 'train_samples_per_second': 0.909, 'train_steps_per_second': 0.057, 'train_loss': 1.115165615081787, 'epoch': 0.3}
[2024-06-14 16:11:47,579][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (37, 0.0, {}, 69311.17910199822)
[2024-06-14 16:11:48,287][flwr][INFO] - fit progress: (37, 0.0, {}, 69311.17910199822)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 16:11:48,288][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 16:11:48,288][flwr][INFO] - 
[92mINFO [0m:      [ROUND 38]
[2024-06-14 16:11:48,288][flwr][INFO] - [ROUND 38]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 16:11:48,288][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.16s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.64s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.88s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.67s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.00s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.44s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.81s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.61s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:30, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.05s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:08, 18.33s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:44, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:06, 16.65s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.18s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:55, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:46, 17.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.42s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:50, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.80s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:42, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:52, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.18s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:27, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.10s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:11<01:44, 11.56s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:04, 15.62s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:45, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:29, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:12, 18.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:54, 18.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:36, 18.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.50s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1163, 'grad_norm': 0.8852034211158752, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7314, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.116321086883545, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1578, 'grad_norm': 0.6639028191566467, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0526, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.1577959060668945, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1305, 'grad_norm': 0.7012913823127747, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.8611, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.1304906845092773, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1538, 'grad_norm': 0.6955482959747314, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.1791, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.1537718772888184, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1313, 'grad_norm': 0.6458907723426819, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.1275, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.1312570571899414, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.111, 'grad_norm': 0.5818585753440857, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.8065, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1109807968139649, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0726, 'grad_norm': 0.8116955757141113, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7375, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.0726057052612306, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1439, 'grad_norm': 0.7136716246604919, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.2389, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.058, 'train_loss': 1.1439496994018554, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1129, 'grad_norm': 0.6600775122642517, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.0612, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1128610610961913, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1366, 'grad_norm': 0.6649924516677856, 'learning_rate': 7.640268628175424e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.0445, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.057, 'train_loss': 1.1366379737854004, 'epoch': 0.3}
[2024-06-14 16:43:03,579][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (38, 0.0, {}, 71187.1557409952)
[2024-06-14 16:43:04,264][flwr][INFO] - fit progress: (38, 0.0, {}, 71187.1557409952)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 16:43:04,264][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 16:43:04,264][flwr][INFO] - 
[92mINFO [0m:      [ROUND 39]
[2024-06-14 16:43:04,264][flwr][INFO] - [ROUND 39]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 16:43:04,264][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.96s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.15s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:07, 18.29s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:39, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:22, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:00, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.34s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.21s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:59, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:10, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.05s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:53, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:23, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:32, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:16, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:04, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:45, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:52, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.44s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:04, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:16, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:05, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:45, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:01<00:28, 14.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.11s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.95s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:15, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:47, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.13s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0816, 'grad_norm': 0.7030262351036072, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.9132, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.061, 'train_loss': 1.081636333465576, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1346, 'grad_norm': 0.8759421110153198, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4457, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1345951080322265, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.156, 'grad_norm': 0.9096487164497375, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.8488, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.061, 'train_loss': 1.156019401550293, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0842, 'grad_norm': 0.6366032958030701, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.6996, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.058, 'train_loss': 1.0842183113098145, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1073, 'grad_norm': 0.6727205514907837, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.1312, 'train_samples_per_second': 0.919, 'train_steps_per_second': 0.057, 'train_loss': 1.1072957992553711, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.112, 'grad_norm': 0.6484121084213257, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.6289, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.057, 'train_loss': 1.1120332717895507, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1605, 'grad_norm': 0.9020312428474426, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.5199, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.056, 'train_loss': 1.1605161666870116, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0544, 'grad_norm': 0.6734102964401245, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.5638, 'train_samples_per_second': 1.009, 'train_steps_per_second': 0.063, 'train_loss': 1.0544242858886719, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1634, 'grad_norm': 0.7137384414672852, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.9424, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.1634368896484375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0923, 'grad_norm': 1.0874338150024414, 'learning_rate': 6.622425551993166e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6969, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.0922820091247558, 'epoch': 0.3}
[2024-06-14 17:14:41,567][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (39, 0.0, {}, 73085.175771632)
[2024-06-14 17:14:42,284][flwr][INFO] - fit progress: (39, 0.0, {}, 73085.175771632)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 17:14:42,284][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 17:14:42,284][flwr][INFO] - 
[92mINFO [0m:      [ROUND 40]
[2024-06-14 17:14:42,284][flwr][INFO] - [ROUND 40]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 17:14:42,284][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.27s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.55s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:57, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Map:   0%|          | 0/520 [00:00<?, ? examples/s]
[36m(ClientAppActor pid=979026)[0m Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2608.79 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 2476.47 examples/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:42, 12.75s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:38, 14.03s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:34, 15.78s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.52s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:31<01:01, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:49, 16.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:04<00:31, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:02, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.10s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:50, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:33, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.43s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.63s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.10s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.13s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.64s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:18, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:41, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.99s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.38s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.86s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:54, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:36, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:55<00:00, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.04s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.96s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.90s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.73s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.62s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:07, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0987, 'grad_norm': 0.7837052941322327, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9539, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.098727035522461, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.151, 'grad_norm': 0.676994800567627, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.7462, 'train_samples_per_second': 1.014, 'train_steps_per_second': 0.063, 'train_loss': 1.1510183334350585, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.103, 'grad_norm': 0.7501962780952454, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.2767, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.1029809951782226, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1153, 'grad_norm': 0.8820121884346008, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7743, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1153293609619142, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1029, 'grad_norm': 0.71059250831604, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.065, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.062, 'train_loss': 1.1029143333435059, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1068, 'grad_norm': 0.6657627820968628, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3347, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.106813621520996, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1229, 'grad_norm': 0.8665034770965576, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 175.5528, 'train_samples_per_second': 0.911, 'train_steps_per_second': 0.057, 'train_loss': 1.1228594779968262, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1307, 'grad_norm': 0.7226966619491577, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.7178, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.130681324005127, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0899, 'grad_norm': 0.7632681727409363, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.6527, 'train_samples_per_second': 0.949, 'train_steps_per_second': 0.059, 'train_loss': 1.0899333953857422, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1013, 'grad_norm': 0.6916534900665283, 'learning_rate': 5.6790836378137905e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.7347, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.101252841949463, 'epoch': 0.3}
[2024-06-14 17:45:52,025][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.37s/it]
[92mINFO [0m:      fit progress: (40, 0.0, {}, 74972.87996120518)
[2024-06-14 17:46:09,988][flwr][INFO] - fit progress: (40, 0.0, {}, 74972.87996120518)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 17:46:09,989][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 17:46:09,989][flwr][INFO] - 
[92mINFO [0m:      [ROUND 41]
[2024-06-14 17:46:09,989][flwr][INFO] - [ROUND 41]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 17:46:09,989][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.26s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.94s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.69s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:09, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:49, 16.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.92s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:43, 14.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:37, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:16, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<01:01, 15.43s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:47, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.75s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.06s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.82s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<02:05, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:12, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:02, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:47, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:11, 17.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:34, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 16.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.10s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.38s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:38, 16.44s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:18, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.13s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:43, 17.30s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:28, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:11, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.72s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:20<00:35, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:38<00:18, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.88s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.63s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:35, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:02, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.21s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 17.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.83s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.53s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:47, 15.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:09, 13.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:48<00:49, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:34, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.63s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.63s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.13s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.82s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 15.00s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.56s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.43s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:56, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:45, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.46s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.04s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:18, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:41, 14.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.02s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1158, 'grad_norm': 0.6240785717964172, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.883, 'train_samples_per_second': 0.959, 'train_steps_per_second': 0.06, 'train_loss': 1.115789794921875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1001, 'grad_norm': 0.9383781552314758, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.4826, 'train_samples_per_second': 1.016, 'train_steps_per_second': 0.063, 'train_loss': 1.1001172065734863, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1134, 'grad_norm': 0.7070024013519287, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.0184, 'train_samples_per_second': 0.936, 'train_steps_per_second': 0.058, 'train_loss': 1.1133927345275878, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1489, 'grad_norm': 0.6356849670410156, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.493, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1489184379577637, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.097, 'grad_norm': 0.7049280405044556, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.3612, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.057, 'train_loss': 1.097024631500244, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1119, 'grad_norm': 0.6630020141601562, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.1074, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.061, 'train_loss': 1.1118633270263671, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0826, 'grad_norm': 1.0554914474487305, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.7774, 'train_samples_per_second': 1.001, 'train_steps_per_second': 0.063, 'train_loss': 1.0826282501220703, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.182, 'grad_norm': 0.7690631151199341, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.2326, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.1820122718811035, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1157, 'grad_norm': 1.040631651878357, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.445, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'train_loss': 1.1157447814941406, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1059, 'grad_norm': 0.680767834186554, 'learning_rate': 4.813965825200637e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.3377, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.105898094177246, 'epoch': 0.3}
[2024-06-14 18:16:56,966][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (41, 0.0, {}, 76820.55234221113)
[2024-06-14 18:16:57,661][flwr][INFO] - fit progress: (41, 0.0, {}, 76820.55234221113)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 18:16:57,661][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 18:16:57,661][flwr][INFO] - 
[92mINFO [0m:      [ROUND 42]
[2024-06-14 18:16:57,661][flwr][INFO] - [ROUND 42]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 18:16:57,661][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.23s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.93s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.54s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:41, 12.74s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:38, 14.02s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:34, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:31<01:01, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:49<00:49, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:04<00:31, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 16.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.46s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.98s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:15, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.42s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.94s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.40s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:24, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:43, 14.72s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:37, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:16, 15.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<01:01, 15.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:47, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:16, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 14.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.10s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.07s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:10, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.25s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:34, 17.16s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.10s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.63s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:22, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.16s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:50, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.45s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.10s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.78s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:44, 14.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:31, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.32s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1524, 'grad_norm': 0.7022199630737305, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.2008, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.152428436279297, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1812, 'grad_norm': 0.7699659466743469, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.2328, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.1812209129333495, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1501, 'grad_norm': 0.6816495656967163, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.6909, 'train_samples_per_second': 1.015, 'train_steps_per_second': 0.063, 'train_loss': 1.1501150131225586, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1333, 'grad_norm': 0.8778191208839417, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.4649, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.061, 'train_loss': 1.1333015441894532, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1142, 'grad_norm': 0.7605596780776978, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0809, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1142143249511718, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0994, 'grad_norm': 0.9342083930969238, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 157.3972, 'train_samples_per_second': 1.017, 'train_steps_per_second': 0.064, 'train_loss': 1.0993894577026366, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1549, 'grad_norm': 0.9114385843276978, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.7908, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.061, 'train_loss': 1.1549087524414063, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1129, 'grad_norm': 0.5969623923301697, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.6991, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.1128649711608887, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0813, 'grad_norm': 0.7032729387283325, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.7738, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.081336498260498, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0711, 'grad_norm': 0.786733865737915, 'learning_rate': 4.030486338925343e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.2325, 'train_samples_per_second': 0.98, 'train_steps_per_second': 0.061, 'train_loss': 1.0710801124572753, 'epoch': 0.3}
[2024-06-14 18:47:52,312][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (42, 0.0, {}, 78675.93996247603)
[2024-06-14 18:47:53,048][flwr][INFO] - fit progress: (42, 0.0, {}, 78675.93996247603)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 18:47:53,048][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 18:47:53,048][flwr][INFO] - 
[92mINFO [0m:      [ROUND 43]
[2024-06-14 18:47:53,048][flwr][INFO] - [ROUND 43]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 18:47:53,049][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.14s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.86s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.78s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.82s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:20, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:04, 15.51s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:47, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:18, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:06, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:51, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.90s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:25, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:57, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.84s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:21, 16.37s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.60s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:09, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.66s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:40, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:31, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.29s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:18, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:21, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:01, 15.46s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.53s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.76s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:59, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:29, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.46s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.89s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:58, 13.17s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:10, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:51, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:24, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:06, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:50, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.02s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:11, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:35, 13.62s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:33, 15.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:22, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:08, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:33, 16.85s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:17, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 17.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.48s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.60s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:10, 16.29s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:44, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:20, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:34<00:59, 14.90s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:46, 15.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:30, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1713, 'grad_norm': 0.9675983786582947, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.7659, 'train_samples_per_second': 1.021, 'train_steps_per_second': 0.064, 'train_loss': 1.171280288696289, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0744, 'grad_norm': 0.6902498602867126, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.7287, 'train_samples_per_second': 0.954, 'train_steps_per_second': 0.06, 'train_loss': 1.074350929260254, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1609, 'grad_norm': 0.8934211730957031, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.7603, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.1608649253845216, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1005, 'grad_norm': 0.6645106077194214, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.2101, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.1005142211914063, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0976, 'grad_norm': 0.7796502113342285, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.9026, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.0975654602050782, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.129, 'grad_norm': 0.806937038898468, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.8304, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.1289655685424804, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1348, 'grad_norm': 0.647954523563385, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.2999, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1347753524780273, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1272, 'grad_norm': 0.8900738954544067, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.801, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.1272222518920898, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1122, 'grad_norm': 0.5970228314399719, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.6589, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.1121603012084962, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1287, 'grad_norm': 0.7054709196090698, 'learning_rate': 3.331737214582526e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.8309, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.064, 'train_loss': 1.1286655426025392, 'epoch': 0.3}
[2024-06-14 19:18:46,301][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (43, 0.0, {}, 80529.90459959721)
[2024-06-14 19:18:47,013][flwr][INFO] - fit progress: (43, 0.0, {}, 80529.90459959721)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 19:18:47,013][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 19:18:47,013][flwr][INFO] - 
[92mINFO [0m:      [ROUND 44]
[2024-06-14 19:18:47,013][flwr][INFO] - [ROUND 44]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 19:18:47,013][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:25, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:30, 15.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.45s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.51s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:16, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.91s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.41s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:32<01:32, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:30, 15.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.53s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.59s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:42, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:28, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:53, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:34, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.92s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.94s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.11s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.08s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.51s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.33s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:50, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:40, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:27, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:32, 16.42s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.65s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:19, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.94s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:44, 14.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:35, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:17, 15.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:34, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:16, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.62s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:58, 14.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.99s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.17s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:15, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:47, 17.94s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:12, 18.04s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:47, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.51s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1388, 'grad_norm': 0.7624157071113586, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.0646, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.1387590408325194, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0966, 'grad_norm': 0.5988929867744446, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9926, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.0965816497802734, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1806, 'grad_norm': 0.7724286317825317, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.1919, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.06, 'train_loss': 1.1806035041809082, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0659, 'grad_norm': 0.7711564898490906, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 176.4522, 'train_samples_per_second': 0.907, 'train_steps_per_second': 0.057, 'train_loss': 1.0658617973327638, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1136, 'grad_norm': 0.7621185183525085, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0707, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1136113166809083, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0807, 'grad_norm': 0.6995370388031006, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.7589, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.058, 'train_loss': 1.0806808471679688, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1098, 'grad_norm': 0.8189056515693665, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0409, 'train_samples_per_second': 1.012, 'train_steps_per_second': 0.063, 'train_loss': 1.109764003753662, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0932, 'grad_norm': 0.5920228958129883, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6565, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.0931902885437013, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1624, 'grad_norm': 0.719821572303772, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.892, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.162369728088379, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0752, 'grad_norm': 0.8038054704666138, 'learning_rate': 2.720476095737842e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.0648, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.0752100944519043, 'epoch': 0.3}
[2024-06-14 19:50:07,774][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (44, 0.0, {}, 82411.34092157008)
[2024-06-14 19:50:08,449][flwr][INFO] - fit progress: (44, 0.0, {}, 82411.34092157008)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 19:50:08,449][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 19:50:08,449][flwr][INFO] - 
[92mINFO [0m:      [ROUND 45]
[2024-06-14 19:50:08,449][flwr][INFO] - [ROUND 45]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 19:50:08,450][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.15s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.61s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.81s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.46s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:09<01:28,  9.79s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:59, 14.89s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:24, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.27s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.56s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:56, 12.93s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:50, 13.85s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:51, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:39, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:26, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:08, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:52, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:34, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:42, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:28, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.44s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:53, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:34, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.58s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.47s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.78s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:59, 17.00s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.78s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:44, 14.90s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:31, 15.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.01s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.44s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:13<01:57, 13.03s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:55, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.57s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:15, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:33, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:16, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 17.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.18s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.95s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.57s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:47, 17.92s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:30, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:48<01:12, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:24<00:35, 18.00s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:41<00:17, 17.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.78s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.47s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.03s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.30s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:13, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:40, 14.31s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:35, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:17, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:00, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:48, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:06<00:31, 15.74s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:16, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.45s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:07, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:59<00:51, 17.14s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.77s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.18s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0914, 'grad_norm': 1.0989644527435303, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6881, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.061, 'train_loss': 1.0913583755493164, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1351, 'grad_norm': 0.667479395866394, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.5532, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'train_loss': 1.1350750923156738, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1133, 'grad_norm': 0.762696385383606, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.0907, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.059, 'train_loss': 1.1132516860961914, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0655, 'grad_norm': 0.7712964415550232, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 176.4455, 'train_samples_per_second': 0.907, 'train_steps_per_second': 0.057, 'train_loss': 1.0654654502868652, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0703, 'grad_norm': 0.7948028445243835, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.1706, 'train_samples_per_second': 0.981, 'train_steps_per_second': 0.061, 'train_loss': 1.0703446388244628, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1091, 'grad_norm': 0.561397135257721, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.776, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'train_loss': 1.1091061592102052, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1453, 'grad_norm': 0.8679022192955017, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 177.7859, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.056, 'train_loss': 1.14525146484375, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1145, 'grad_norm': 0.8760625720024109, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7177, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1145439147949219, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0688, 'grad_norm': 0.8620157241821289, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.2478, 'train_samples_per_second': 1.011, 'train_steps_per_second': 0.063, 'train_loss': 1.068820095062256, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0691, 'grad_norm': 0.5953626036643982, 'learning_rate': 2.1991153507687385e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 171.7686, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.058, 'train_loss': 1.069117259979248, 'epoch': 0.3}
[2024-06-14 20:21:34,816][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.83s/it]
[92mINFO [0m:      fit progress: (45, 0.0, {}, 84316.74942587921)
[2024-06-14 20:21:53,858][flwr][INFO] - fit progress: (45, 0.0, {}, 84316.74942587921)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 20:21:53,858][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 20:21:53,858][flwr][INFO] - 
[92mINFO [0m:      [ROUND 46]
[2024-06-14 20:21:53,858][flwr][INFO] - [ROUND 46]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 20:21:53,859][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.35s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:05, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:45, 17.53s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:20, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.76s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.02s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:34, 15.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:03, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.07s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.61s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.31s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:18, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:41, 14.50s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:06, 16.57s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.06s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.54s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.70s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:36, 16.13s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:23, 16.64s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:14<00:33, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:16, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.36s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.33s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.76s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:50, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:15, 15.01s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.09s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.09s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.82s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:37<02:28, 18.52s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:30<01:28, 17.79s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:10, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:06<00:53, 17.91s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:40<00:17, 17.54s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:58<00:00, 17.88s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:44, 18.25s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:51, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:17, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<01:04, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:48, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:03<00:28, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.78s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.56s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:30, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:15, 16.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:03, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:10<01:47, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:30, 18.19s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:12, 18.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:48, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.64s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.58s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:10, 14.52s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:09, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<02:00, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:46, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:08, 17.18s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:52, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:35<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.50s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.50s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.35s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.075, 'grad_norm': 0.8321689367294312, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.3403, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.058, 'train_loss': 1.0749738693237305, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0807, 'grad_norm': 0.8210818767547607, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0774, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.062, 'train_loss': 1.0806735038757325, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1049, 'grad_norm': 0.6759366393089294, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.607, 'train_samples_per_second': 0.984, 'train_steps_per_second': 0.061, 'train_loss': 1.10494966506958, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1039, 'grad_norm': 0.8745446801185608, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.4425, 'train_samples_per_second': 0.939, 'train_steps_per_second': 0.059, 'train_loss': 1.1039461135864257, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1562, 'grad_norm': 0.6855188012123108, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.463, 'train_samples_per_second': 1.01, 'train_steps_per_second': 0.063, 'train_loss': 1.1562457084655762, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.159, 'grad_norm': 0.9026332497596741, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 178.7967, 'train_samples_per_second': 0.895, 'train_steps_per_second': 0.056, 'train_loss': 1.159006118774414, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1531, 'grad_norm': 0.6555076837539673, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.3859, 'train_samples_per_second': 1.023, 'train_steps_per_second': 0.064, 'train_loss': 1.1530703544616698, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0887, 'grad_norm': 0.765889048576355, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.8013, 'train_samples_per_second': 0.948, 'train_steps_per_second': 0.059, 'train_loss': 1.0887416839599608, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1619, 'grad_norm': 0.7207897305488586, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.359, 'train_samples_per_second': 0.928, 'train_steps_per_second': 0.058, 'train_loss': 1.1619294166564942, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1283, 'grad_norm': 0.8099973201751709, 'learning_rate': 1.7697125523485413e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.4663, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.058, 'train_loss': 1.128261184692383, 'epoch': 0.3}
[2024-06-14 20:53:20,868][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (46, 0.0, {}, 86204.42700946401)
[2024-06-14 20:53:21,535][flwr][INFO] - fit progress: (46, 0.0, {}, 86204.42700946401)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 20:53:21,535][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 20:53:21,536][flwr][INFO] - 
[92mINFO [0m:      [ROUND 47]
[2024-06-14 20:53:21,536][flwr][INFO] - [ROUND 47]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 20:53:21,536][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.64s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.05s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:22, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:42<01:07, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:00<00:51, 17.19s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:18<00:35, 17.60s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:52<00:00, 17.21s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.39s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:58, 16.87s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.12s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.13s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.40s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.14s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.68s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:09, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:58, 16.99s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:45, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:25, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:08, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.54s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.93s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.70s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.14s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.51s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:59, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:40, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:12, 14.49s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:00, 15.14s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:48, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.94s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.08s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.94s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:16, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:51, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:23, 13.85s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:13, 14.78s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:02, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:15, 15.30s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:36<00:00, 15.68s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.14s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.79s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:34, 15.67s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:09, 17.27s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:31, 15.85s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:16, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.51s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.12s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:04, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:45, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:52, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.85s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.86s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:18, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:41, 14.47s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:19, 15.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.17s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:32, 16.01s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.82s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.08s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:49, 12.18s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:06, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:57, 16.81s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:22, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.23s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:52, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.31s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0688, 'grad_norm': 0.5976384282112122, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 172.0611, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.058, 'train_loss': 1.068814754486084, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1629, 'grad_norm': 0.7155267596244812, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.2396, 'train_samples_per_second': 0.992, 'train_steps_per_second': 0.062, 'train_loss': 1.1629058837890625, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1148, 'grad_norm': 0.6141542196273804, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.97, 'train_samples_per_second': 0.958, 'train_steps_per_second': 0.06, 'train_loss': 1.114840030670166, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1175, 'grad_norm': 0.6787000894546509, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.4179, 'train_samples_per_second': 0.956, 'train_steps_per_second': 0.06, 'train_loss': 1.1175043106079101, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1707, 'grad_norm': 0.9617273807525635, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 156.7573, 'train_samples_per_second': 1.021, 'train_steps_per_second': 0.064, 'train_loss': 1.170703411102295, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1547, 'grad_norm': 0.630399227142334, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.8125, 'train_samples_per_second': 0.965, 'train_steps_per_second': 0.06, 'train_loss': 1.1546920776367187, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1107, 'grad_norm': 0.6475166082382202, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.6337, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.057, 'train_loss': 1.1106739044189453, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1001, 'grad_norm': 0.6911742687225342, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6955, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.1000922203063965, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1047, 'grad_norm': 0.6788896918296814, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.2975, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.104683780670166, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1001, 'grad_norm': 0.6815776824951172, 'learning_rate': 1.4339623571471263e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.1194, 'train_samples_per_second': 0.981, 'train_steps_per_second': 0.061, 'train_loss': 1.1001407623291015, 'epoch': 0.3}
[2024-06-14 21:24:19,977][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (47, 0.0, {}, 88063.5806233401)
[2024-06-14 21:24:20,689][flwr][INFO] - fit progress: (47, 0.0, {}, 88063.5806233401)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 21:24:20,689][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 21:24:20,689][flwr][INFO] - 
[92mINFO [0m:      [ROUND 48]
[2024-06-14 21:24:20,689][flwr][INFO] - [ROUND 48]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 21:24:20,689][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.90s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:11, 14.61s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:58, 14.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:37, 16.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:18<01:19, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:07, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:52, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:35, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.10s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.16s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:53, 16.21s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:36, 16.14s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:20<01:15, 15.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:51, 17.01s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:15, 15.17s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.40s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.20s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<02:01, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:28<01:27, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:04<00:53, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:21<00:34, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:37<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 15.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:12<01:49, 12.16s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:06, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:57, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:06<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:22, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:52, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 15.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:43<00:00, 16.31s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.83s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:32, 16.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.84s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:06, 18.12s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:41, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:27, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.98s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:47, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.82s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:38, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.89s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.60s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:33, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.12s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:28, 16.46s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:21, 17.63s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:56, 16.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:03<01:31, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:08, 17.09s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:53<00:47, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:10<00:32, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.29s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.81s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.29s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.48s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.59s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.10s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:52<01:58, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:33, 15.58s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:04, 16.22s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:31, 15.92s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 17.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.69s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.19s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:11, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:01, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:16, 15.36s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:05, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:47, 15.83s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:33, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:28<00:16, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.98s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.67s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0927, 'grad_norm': 0.5985148549079895, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.6308, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.062, 'train_loss': 1.0927361488342284, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0962, 'grad_norm': 0.5950638651847839, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.9713, 'train_samples_per_second': 0.976, 'train_steps_per_second': 0.061, 'train_loss': 1.0961630821228028, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1142, 'grad_norm': 0.8737109899520874, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7076, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.1141977310180664, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0999, 'grad_norm': 0.6823356747627258, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 163.0972, 'train_samples_per_second': 0.981, 'train_steps_per_second': 0.061, 'train_loss': 1.0999117851257325, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0457, 'grad_norm': 0.7208459377288818, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.95, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.045733642578125, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1896, 'grad_norm': 0.8551680445671082, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0043, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'train_loss': 1.1896315574645997, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.134, 'grad_norm': 0.6541250348091125, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.2839, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'train_loss': 1.1339924812316895, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1517, 'grad_norm': 0.7066748142242432, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.2064, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.151716423034668, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0884, 'grad_norm': 0.6669597029685974, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.9602, 'train_samples_per_second': 0.953, 'train_steps_per_second': 0.06, 'train_loss': 1.088384246826172, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1041, 'grad_norm': 0.7368184924125671, 'learning_rate': 1.1931898177952948e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.7407, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.06, 'train_loss': 1.104142665863037, 'epoch': 0.3}
[2024-06-14 21:55:25,393][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (48, 0.0, {}, 89929.01053268905)
[2024-06-14 21:55:26,119][flwr][INFO] - fit progress: (48, 0.0, {}, 89929.01053268905)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 21:55:26,119][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 21:55:26,119][flwr][INFO] - 
[92mINFO [0m:      [ROUND 49]
[2024-06-14 21:55:26,119][flwr][INFO] - [ROUND 49]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 21:55:26,119][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.28s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.95s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:28, 14.83s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.22s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:05, 16.32s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:49, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:34, 17.13s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:25<00:17, 17.05s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.72s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.23s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:14<02:12, 14.67s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.00s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:56, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:31, 15.25s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:12, 14.43s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:33<01:03, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.67s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:07<00:32, 16.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:19<00:14, 14.97s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 16.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:38<00:00, 15.80s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.78s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:18, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:08, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.73s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:26, 17.35s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.22s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.35s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.60s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 15.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.27s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.38s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.39s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<01:58, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.24s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:19, 15.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:36<00:59, 14.94s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:48, 16.09s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:24<00:15, 15.55s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 15.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:41<00:00, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.94s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:20, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.41s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:39, 16.62s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:25, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:10, 17.62s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:53, 17.89s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:30, 15.28s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:46<00:00, 16.61s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.51s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.08s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:29, 16.58s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.98s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:49, 15.71s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:40, 16.80s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:24<01:26, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:41<01:09, 17.37s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:51<00:44, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:30, 15.41s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:15, 15.57s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.91s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.77s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.16s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:09<01:27,  9.78s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:59, 14.88s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:46, 15.18s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:36, 16.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:19<01:24, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:49, 16.66s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:32, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:21<00:15, 15.26s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 16.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:39<00:00, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.85s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:10, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:23, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.28s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.12s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.50s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:04, 17.85s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:45, 17.65s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:26<01:23, 16.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:07, 16.91s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:02<00:52, 17.41s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:17<00:33, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:17, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.68s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:54<00:00, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.74s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.29s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:22, 17.81s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:11<01:46, 17.71s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:29<01:29, 17.98s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:47<01:11, 17.76s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:03<00:51, 17.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:22<00:35, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:36<00:16, 16.72s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 16.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:53<00:00, 17.32s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1294, 'grad_norm': 0.72281813621521, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6921, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.061, 'train_loss': 1.1293655395507813, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1559, 'grad_norm': 0.6896684169769287, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 158.0224, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.063, 'train_loss': 1.1559242248535155, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0998, 'grad_norm': 0.6912171840667725, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.6774, 'train_samples_per_second': 0.984, 'train_steps_per_second': 0.061, 'train_loss': 1.0998369216918946, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1627, 'grad_norm': 0.7161722183227539, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 161.0868, 'train_samples_per_second': 0.993, 'train_steps_per_second': 0.062, 'train_loss': 1.162651252746582, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1382, 'grad_norm': 0.7685825228691101, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 166.0627, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.06, 'train_loss': 1.138235855102539, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.112, 'grad_norm': 0.7557845115661621, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.0517, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.063, 'train_loss': 1.1119670867919922, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1346, 'grad_norm': 0.6630993485450745, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 159.548, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'train_loss': 1.1346482276916503, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1373, 'grad_norm': 0.7528737783432007, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.6325, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.1372772216796876, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1104, 'grad_norm': 0.6474771499633789, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 174.6336, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.057, 'train_loss': 1.1104461669921875, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1252, 'grad_norm': 0.7051306962966919, 'learning_rate': 1.0483451535073467e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 173.2015, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.058, 'train_loss': 1.1252458572387696, 'epoch': 0.3}
[2024-06-14 22:26:16,752][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      fit progress: (49, 0.0, {}, 91780.38290568814)
[2024-06-14 22:26:17,491][flwr][INFO] - fit progress: (49, 0.0, {}, 91780.38290568814)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 22:26:17,491][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 22:26:17,491][flwr][INFO] - 
[92mINFO [0m:      [ROUND 50]
[2024-06-14 22:26:17,491][flwr][INFO] - [ROUND 50]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 100)
[2024-06-14 22:26:17,492][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 100)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.93s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:35<02:21, 17.67s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:55, 16.48s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.69s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:46<01:11, 17.95s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:58<00:48, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:16<00:33, 16.71s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:16, 16.83s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 16.92s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.07s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.99s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.44s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.48s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:55<02:09, 18.46s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:37, 16.33s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:23, 16.76s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:44<01:09, 17.34s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:56<00:46, 15.64s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.45s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:16, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.75s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.52s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.96s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:10, 16.25s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:00, 17.26s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:33, 15.61s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:23, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:51, 17.06s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:34, 17.32s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 17.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.31s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.95s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:27, 18.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:50<01:54, 16.38s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:43, 17.20s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:22<01:19, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:06, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:49, 16.55s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:15<00:33, 16.96s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:33<00:17, 17.43s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.30s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.00s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:16, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:51<02:03, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:09<01:44, 17.46s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:28, 17.77s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:40<01:05, 16.30s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.87s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:32, 16.26s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:27<00:15, 15.73s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.59s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.68s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:16<02:24, 16.08s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:34<02:19, 17.47s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:49<01:52, 16.11s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:42, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:27, 17.55s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:43<01:09, 17.40s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:01<00:53, 17.68s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:19<00:35, 17.75s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:34<00:17, 17.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 15.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.64s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.29s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:33<02:09, 16.23s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:48, 15.44s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:04<01:36, 16.10s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:23<01:24, 16.90s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:39<01:06, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:49, 16.63s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:13<00:33, 16.86s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:31<00:17, 17.36s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:44<00:00, 16.47s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.75s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.70s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:18<02:46, 18.49s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:07, 15.93s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.01s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:34, 15.76s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:23, 16.74s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:03, 15.97s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:54<00:50, 16.78s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:12<00:34, 17.15s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:30<00:17, 17.56s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.51s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.66s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.54s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:17<02:41, 17.97s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:26, 18.26s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.08s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:12<01:49, 18.23s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:27<01:24, 16.88s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:45<01:09, 17.42s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:57<00:46, 15.52s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:30, 15.19s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:16, 16.03s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 15.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:45<00:00, 16.52s/it]
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m `low_cpu_mem_usage` was None, now set to True since model is quantized.
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.73s/it]
[36m(ClientAppActor pid=979026)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.68s/it]
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.
[36m(ClientAppActor pid=979026)[0m 
[36m(ClientAppActor pid=979026)[0m Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(message, FutureWarning)
[36m(ClientAppActor pid=979026)[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
[36m(ClientAppActor pid=979026)[0m To disable this warning, you can either:
[36m(ClientAppActor pid=979026)[0m 	- Avoid using `tokenizers` before the fork if possible
[36m(ClientAppActor pid=979026)[0m 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m /work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
[36m(ClientAppActor pid=979026)[0m dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[36m(ClientAppActor pid=979026)[0m   0%|          | 0/10 [00:00<?, ?it/s]/work/tmehboob_umass_edu/Federated_learning/Flower_fm/fed_learning/LLMs-Llama/llm-flowertune/pnnl-setup/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(ClientAppActor pid=979026)[0m   warnings.warn(
[36m(ClientAppActor pid=979026)[0m  10%|â–ˆ         | 1/10 [00:15<02:15, 15.09s/it]
[36m(ClientAppActor pid=979026)[0m  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:52, 14.12s/it]
[36m(ClientAppActor pid=979026)[0m  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.37s/it]
[36m(ClientAppActor pid=979026)[0m  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:27, 14.63s/it]
[36m(ClientAppActor pid=979026)[0m  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:13<01:14, 14.96s/it]
[36m(ClientAppActor pid=979026)[0m  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:32<01:04, 16.15s/it]
[36m(ClientAppActor pid=979026)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:47, 15.95s/it]
[36m(ClientAppActor pid=979026)[0m  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:32, 16.39s/it]
[36m(ClientAppActor pid=979026)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:23<00:17, 17.04s/it]
[36m(ClientAppActor pid=979026)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 17.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:42<00:00, 16.20s/it]
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0384, 'grad_norm': 0.6058464646339417, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 170.7218, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.059, 'train_loss': 1.038398838043213, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.106, 'grad_norm': 0.6357935667037964, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 167.5466, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.06, 'train_loss': 1.1059884071350097, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1371, 'grad_norm': 0.7524939775466919, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 169.6079, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.059, 'train_loss': 1.1370646476745605, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0885, 'grad_norm': 0.7628843188285828, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.5705, 'train_samples_per_second': 0.949, 'train_steps_per_second': 0.059, 'train_loss': 1.088475227355957, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1546, 'grad_norm': 0.6047259569168091, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.908, 'train_samples_per_second': 0.964, 'train_steps_per_second': 0.06, 'train_loss': 1.1545893669128418, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1515, 'grad_norm': 0.7067472338676453, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 168.182, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.059, 'train_loss': 1.1515295028686523, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0911, 'grad_norm': 1.099105715751648, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 164.6988, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.061, 'train_loss': 1.0911176681518555, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0748, 'grad_norm': 0.7914853692054749, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.1021, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.061, 'train_loss': 1.074805450439453, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.0977, 'grad_norm': 0.7168793082237244, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 165.2192, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.061, 'train_loss': 1.097735595703125, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'loss': 1.1018, 'grad_norm': 0.7014641761779785, 'learning_rate': 1e-06, 'epoch': 0.3}
[36m(ClientAppActor pid=979026)[0m {'train_runtime': 162.0276, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.062, 'train_loss': 1.1017788887023925, 'epoch': 0.3}
[2024-06-14 22:57:25,191][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.37s/it]
[92mINFO [0m:      fit progress: (50, 0.0, {}, 93665.92846897012)
[2024-06-14 22:57:43,037][flwr][INFO] - fit progress: (50, 0.0, {}, 93665.92846897012)
[92mINFO [0m:      configure_evaluate: no clients selected, skipping evaluation
[2024-06-14 22:57:43,037][flwr][INFO] - configure_evaluate: no clients selected, skipping evaluation
[92mINFO [0m:      
[2024-06-14 22:57:43,054][flwr][INFO] - 
[92mINFO [0m:      [SUMMARY]
[2024-06-14 22:57:43,054][flwr][INFO] - [SUMMARY]
[92mINFO [0m:      Run finished 50 round(s) in 93665.93s
[2024-06-14 22:57:43,055][flwr][INFO] - Run finished 50 round(s) in 93665.93s
[92mINFO [0m:      	History (loss, centralized):
[2024-06-14 22:57:43,057][flwr][INFO] - 	History (loss, centralized):
[92mINFO [0m:      		round 0: 0.0
[2024-06-14 22:57:43,057][flwr][INFO] - 		round 0: 0.0
[92mINFO [0m:      		round 1: 0.0
[2024-06-14 22:57:43,057][flwr][INFO] - 		round 1: 0.0
[92mINFO [0m:      		round 2: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 2: 0.0
[92mINFO [0m:      		round 3: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 3: 0.0
[92mINFO [0m:      		round 4: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 4: 0.0
[92mINFO [0m:      		round 5: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 5: 0.0
[92mINFO [0m:      		round 6: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 6: 0.0
[92mINFO [0m:      		round 7: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 7: 0.0
[92mINFO [0m:      		round 8: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 8: 0.0
[92mINFO [0m:      		round 9: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 9: 0.0
[92mINFO [0m:      		round 10: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 10: 0.0
[92mINFO [0m:      		round 11: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 11: 0.0
[92mINFO [0m:      		round 12: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 12: 0.0
[92mINFO [0m:      		round 13: 0.0
[2024-06-14 22:57:43,058][flwr][INFO] - 		round 13: 0.0
[92mINFO [0m:      		round 14: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 14: 0.0
[92mINFO [0m:      		round 15: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 15: 0.0
[92mINFO [0m:      		round 16: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 16: 0.0
[92mINFO [0m:      		round 17: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 17: 0.0
[92mINFO [0m:      		round 18: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 18: 0.0
[92mINFO [0m:      		round 19: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 19: 0.0
[92mINFO [0m:      		round 20: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 20: 0.0
[92mINFO [0m:      		round 21: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 21: 0.0
[92mINFO [0m:      		round 22: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 22: 0.0
[92mINFO [0m:      		round 23: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 23: 0.0
[92mINFO [0m:      		round 24: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 24: 0.0
[92mINFO [0m:      		round 25: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 25: 0.0
[92mINFO [0m:      		round 26: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 26: 0.0
[92mINFO [0m:      		round 27: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 27: 0.0
[92mINFO [0m:      		round 28: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 28: 0.0
[92mINFO [0m:      		round 29: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 29: 0.0
[92mINFO [0m:      		round 30: 0.0
[2024-06-14 22:57:43,059][flwr][INFO] - 		round 30: 0.0
[92mINFO [0m:      		round 31: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 31: 0.0
[92mINFO [0m:      		round 32: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 32: 0.0
[92mINFO [0m:      		round 33: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 33: 0.0
[92mINFO [0m:      		round 34: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 34: 0.0
[92mINFO [0m:      		round 35: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 35: 0.0
[92mINFO [0m:      		round 36: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 36: 0.0
[92mINFO [0m:      		round 37: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 37: 0.0
[92mINFO [0m:      		round 38: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 38: 0.0
[92mINFO [0m:      		round 39: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 39: 0.0
[92mINFO [0m:      		round 40: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 40: 0.0
[92mINFO [0m:      		round 41: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 41: 0.0
[92mINFO [0m:      		round 42: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 42: 0.0
[92mINFO [0m:      		round 43: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 43: 0.0
[92mINFO [0m:      		round 44: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 44: 0.0
[92mINFO [0m:      		round 45: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 45: 0.0
[92mINFO [0m:      		round 46: 0.0
[2024-06-14 22:57:43,060][flwr][INFO] - 		round 46: 0.0
[92mINFO [0m:      		round 47: 0.0
[2024-06-14 22:57:43,061][flwr][INFO] - 		round 47: 0.0
[92mINFO [0m:      		round 48: 0.0
[2024-06-14 22:57:43,061][flwr][INFO] - 		round 48: 0.0
[92mINFO [0m:      		round 49: 0.0
[2024-06-14 22:57:43,061][flwr][INFO] - 		round 49: 0.0
[92mINFO [0m:      		round 50: 0.0
[2024-06-14 22:57:43,061][flwr][INFO] - 		round 50: 0.0
[92mINFO [0m:      	History (metrics, distributed, fit):
[2024-06-14 22:57:43,061][flwr][INFO] - 	History (metrics, distributed, fit):
[92mINFO [0m:      	{'train_loss': [(1, 1.327775707244873),
[2024-06-14 22:57:43,061][flwr][INFO] - 	{'train_loss': [(1, 1.327775707244873),
[92mINFO [0m:      	                (2, 1.2313295650482177),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (2, 1.2313295650482177),
[92mINFO [0m:      	                (3, 1.196529188156128),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (3, 1.196529188156128),
[92mINFO [0m:      	                (4, 1.1737658882141113),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (4, 1.1737658882141113),
[92mINFO [0m:      	                (5, 1.177951192855835),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (5, 1.177951192855835),
[92mINFO [0m:      	                (6, 1.1629403686523438),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (6, 1.1629403686523438),
[92mINFO [0m:      	                (7, 1.1645072555541993),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (7, 1.1645072555541993),
[92mINFO [0m:      	                (8, 1.1494619178771972),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (8, 1.1494619178771972),
[92mINFO [0m:      	                (9, 1.1402267837524414),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (9, 1.1402267837524414),
[92mINFO [0m:      	                (10, 1.1624973011016846),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (10, 1.1624973011016846),
[92mINFO [0m:      	                (11, 1.134881944656372),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (11, 1.134881944656372),
[92mINFO [0m:      	                (12, 1.1499143314361573),
[2024-06-14 22:57:43,061][flwr][INFO] - 	                (12, 1.1499143314361573),
[92mINFO [0m:      	                (13, 1.1377803802490234),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (13, 1.1377803802490234),
[92mINFO [0m:      	                (14, 1.129015474319458),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (14, 1.129015474319458),
[92mINFO [0m:      	                (15, 1.1473841571807861),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (15, 1.1473841571807861),
[92mINFO [0m:      	                (16, 1.1339519911283622),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (16, 1.1339519911283622),
[92mINFO [0m:      	                (17, 1.129887804795265),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (17, 1.129887804795265),
[92mINFO [0m:      	                (18, 1.133180913925171),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (18, 1.133180913925171),
[92mINFO [0m:      	                (19, 1.1520137691497803),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (19, 1.1520137691497803),
[92mINFO [0m:      	                (20, 1.1196159553527831),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (20, 1.1196159553527831),
[92mINFO [0m:      	                (21, 1.1129858589172363),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (21, 1.1129858589172363),
[92mINFO [0m:      	                (22, 1.1235265827178955),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (22, 1.1235265827178955),
[92mINFO [0m:      	                (23, 1.131557423324453),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (23, 1.131557423324453),
[92mINFO [0m:      	                (24, 1.1256285095214844),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (24, 1.1256285095214844),
[92mINFO [0m:      	                (25, 1.1077358055114745),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (25, 1.1077358055114745),
[92mINFO [0m:      	                (26, 1.1291633319854737),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (26, 1.1291633319854737),
[92mINFO [0m:      	                (27, 1.125048360824585),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (27, 1.125048360824585),
[92mINFO [0m:      	                (28, 1.1238890647888184),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (28, 1.1238890647888184),
[92mINFO [0m:      	                (29, 1.1273155117034912),
[2024-06-14 22:57:43,062][flwr][INFO] - 	                (29, 1.1273155117034912),
[92mINFO [0m:      	                (30, 1.11568133354187),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (30, 1.11568133354187),
[92mINFO [0m:      	                (31, 1.1193755626678468),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (31, 1.1193755626678468),
[92mINFO [0m:      	                (32, 1.1106296539306642),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (32, 1.1106296539306642),
[92mINFO [0m:      	                (33, 1.1286955311215399),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (33, 1.1286955311215399),
[92mINFO [0m:      	                (34, 1.111712188720703),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (34, 1.111712188720703),
[92mINFO [0m:      	                (35, 1.1330115032196044),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (35, 1.1330115032196044),
[92mINFO [0m:      	                (36, 1.1154031562805176),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (36, 1.1154031562805176),
[92mINFO [0m:      	                (37, 1.1277527904510498),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (37, 1.1277527904510498),
[92mINFO [0m:      	                (38, 1.1266671848297118),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (38, 1.1266671848297118),
[92mINFO [0m:      	                (39, 1.114645757675171),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (39, 1.114645757675171),
[92mINFO [0m:      	                (40, 1.1122510719299317),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (40, 1.1122510719299317),
[92mINFO [0m:      	                (41, 1.1173389530181885),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (41, 1.1173389530181885),
[92mINFO [0m:      	                (42, 1.1250860023498535),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (42, 1.1250860023498535),
[92mINFO [0m:      	                (43, 1.1236364841461182),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (43, 1.1236364841461182),
[92mINFO [0m:      	                (44, 1.1116632270812987),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (44, 1.1116632270812987),
[92mINFO [0m:      	                (45, 1.098233413696289),
[2024-06-14 22:57:43,063][flwr][INFO] - 	                (45, 1.098233413696289),
[92mINFO [0m:      	                (46, 1.1211797618865966),
[2024-06-14 22:57:43,064][flwr][INFO] - 	                (46, 1.1211797618865966),
[92mINFO [0m:      	                (47, 1.1205051136016846),
[2024-06-14 22:57:43,064][flwr][INFO] - 	                (47, 1.1205051136016846),
[92mINFO [0m:      	                (48, 1.1116609764099121),
[2024-06-14 22:57:43,064][flwr][INFO] - 	                (48, 1.1116609764099121),
[92mINFO [0m:      	                (49, 1.1305598354339599),
[2024-06-14 22:57:43,064][flwr][INFO] - 	                (49, 1.1305598354339599),
[92mINFO [0m:      	                (50, 1.104148359298706)]}
[2024-06-14 22:57:43,064][flwr][INFO] - 	                (50, 1.104148359298706)]}
[92mINFO [0m:      
[2024-06-14 22:57:43,064][flwr][INFO] - 
................
History (loss, centralized):
	round 0: 0.0
	round 1: 0.0
	round 2: 0.0
	round 3: 0.0
	round 4: 0.0
	round 5: 0.0
	round 6: 0.0
	round 7: 0.0
	round 8: 0.0
	round 9: 0.0
	round 10: 0.0
	round 11: 0.0
	round 12: 0.0
	round 13: 0.0
	round 14: 0.0
	round 15: 0.0
	round 16: 0.0
	round 17: 0.0
	round 18: 0.0
	round 19: 0.0
	round 20: 0.0
	round 21: 0.0
	round 22: 0.0
	round 23: 0.0
	round 24: 0.0
	round 25: 0.0
	round 26: 0.0
	round 27: 0.0
	round 28: 0.0
	round 29: 0.0
	round 30: 0.0
	round 31: 0.0
	round 32: 0.0
	round 33: 0.0
	round 34: 0.0
	round 35: 0.0
	round 36: 0.0
	round 37: 0.0
	round 38: 0.0
	round 39: 0.0
	round 40: 0.0
	round 41: 0.0
	round 42: 0.0
	round 43: 0.0
	round 44: 0.0
	round 45: 0.0
	round 46: 0.0
	round 47: 0.0
	round 48: 0.0
	round 49: 0.0
	round 50: 0.0
History (metrics, distributed, fit):
{'train_loss': [(1, 1.327775707244873),
                (2, 1.2313295650482177),
                (3, 1.196529188156128),
                (4, 1.1737658882141113),
                (5, 1.177951192855835),
                (6, 1.1629403686523438),
                (7, 1.1645072555541993),
                (8, 1.1494619178771972),
                (9, 1.1402267837524414),
                (10, 1.1624973011016846),
                (11, 1.134881944656372),
                (12, 1.1499143314361573),
                (13, 1.1377803802490234),
                (14, 1.129015474319458),
                (15, 1.1473841571807861),
                (16, 1.1339519911283622),
                (17, 1.129887804795265),
                (18, 1.133180913925171),
                (19, 1.1520137691497803),
                (20, 1.1196159553527831),
                (21, 1.1129858589172363),
                (22, 1.1235265827178955),
                (23, 1.131557423324453),
                (24, 1.1256285095214844),
                (25, 1.1077358055114745),
                (26, 1.1291633319854737),
                (27, 1.125048360824585),
                (28, 1.1238890647888184),
                (29, 1.1273155117034912),
                (30, 1.11568133354187),
                (31, 1.1193755626678468),
                (32, 1.1106296539306642),
                (33, 1.1286955311215399),
                (34, 1.111712188720703),
                (35, 1.1330115032196044),
                (36, 1.1154031562805176),
                (37, 1.1277527904510498),
                (38, 1.1266671848297118),
                (39, 1.114645757675171),
                (40, 1.1122510719299317),
                (41, 1.1173389530181885),
                (42, 1.1250860023498535),
                (43, 1.1236364841461182),
                (44, 1.1116632270812987),
                (45, 1.098233413696289),
                (46, 1.1211797618865966),
                (47, 1.1205051136016846),
                (48, 1.1116609764099121),
                (49, 1.1305598354339599),
                (50, 1.104148359298706)]}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
